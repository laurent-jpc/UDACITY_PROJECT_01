{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- PROJECT #01 / START ---------- \n",
      "\n",
      "INFO! Use selected dataset and realize appropriate analysis\n",
      "> Read data\n",
      "  - Input file: C:\\Users\\to202835\\Documents\\exploitation\\formation\\db_covid19\\donnees-hospitalieres-covid-19-dep-france.csv\n",
      "> Dummy cat. data\n",
      "> Clean df\n",
      "  > Manage invalid data\n",
      "> Split data into X/y\n",
      "> Get model\n",
      "  > Split Train / Test\n",
      "  > Type of data\n",
      "  > Modeling\n",
      "  > Metrics\n",
      "    - r2_scores_train: 0.9761588302089482\n",
      "    - r2_scores_test: 0.9755838001373758\n",
      "> Processing for providing stats\n",
      "  > Define impact of variables on the model:\n",
      "                                     est_int         coefs     abs_coefs\n",
      "107                           Sexe_Tous  4.629496e+08  4.629496e+08\n",
      "106                          Sexe_Homme  4.629496e+08  4.629496e+08\n",
      "105                          Sexe_Femme  4.629495e+08  4.629495e+08\n",
      "60               Code_du_Departement_57  2.284012e+08  2.284012e+08\n",
      "78               Code_du_Departement_75  2.284010e+08  2.284010e+08\n",
      "97               Code_du_Departement_94  2.284009e+08  2.284009e+08\n",
      "62               Code_du_Departement_59  2.284009e+08  2.284009e+08\n",
      "63               Code_du_Departement_60  2.284009e+08  2.284009e+08\n",
      "71               Code_du_Departement_68  2.284008e+08  2.284008e+08\n",
      "98               Code_du_Departement_95  2.284008e+08  2.284008e+08\n",
      "70               Code_du_Departement_67  2.284008e+08  2.284008e+08\n",
      "65               Code_du_Departement_62  2.284008e+08  2.284008e+08\n",
      "83               Code_du_Departement_80  2.284008e+08  2.284008e+08\n",
      "8                Code_du_Departement_06  2.284008e+08  2.284008e+08\n",
      "86               Code_du_Departement_83  2.284008e+08  2.284008e+08\n",
      "81               Code_du_Departement_78  2.284008e+08  2.284008e+08\n",
      "4                Code_du_Departement_02  2.284008e+08  2.284008e+08\n",
      "91               Code_du_Departement_88  2.284008e+08  2.284008e+08\n",
      "80               Code_du_Departement_77  2.284008e+08  2.284008e+08\n",
      "57               Code_du_Departement_54  2.284008e+08  2.284008e+08\n",
      "41               Code_du_Departement_38  2.284008e+08  2.284008e+08\n",
      "45               Code_du_Departement_42  2.284008e+08  2.284008e+08\n",
      "72               Code_du_Departement_69  2.284007e+08  2.284007e+08\n",
      "74               Code_du_Departement_71  2.284007e+08  2.284007e+08\n",
      "79               Code_du_Departement_76  2.284007e+08  2.284007e+08\n",
      "54               Code_du_Departement_51  2.284007e+08  2.284007e+08\n",
      "27               Code_du_Departement_26  2.284007e+08  2.284007e+08\n",
      "93               Code_du_Departement_90  2.284007e+08  2.284007e+08\n",
      "20               Code_du_Departement_18  2.284007e+08  2.284007e+08\n",
      "66               Code_du_Departement_63  2.284007e+08  2.284007e+08\n",
      "22               Code_du_Departement_21  2.284007e+08  2.284007e+08\n",
      "28               Code_du_Departement_27  2.284007e+08  2.284007e+08\n",
      "92               Code_du_Departement_89  2.284007e+08  2.284007e+08\n",
      "5                Code_du_Departement_03  2.284007e+08  2.284007e+08\n",
      "10               Code_du_Departement_08  2.284007e+08  2.284007e+08\n",
      "99              Code_du_Departement_971  2.284007e+08  2.284007e+08\n",
      "96               Code_du_Departement_93  2.284007e+08  2.284007e+08\n",
      "87               Code_du_Departement_84  2.284007e+08  2.284007e+08\n",
      "55               Code_du_Departement_52  2.284007e+08  2.284007e+08\n",
      "76               Code_du_Departement_73  2.284007e+08  2.284007e+08\n",
      "13               Code_du_Departement_11  2.284007e+08  2.284007e+08\n",
      "58               Code_du_Departement_55  2.284007e+08  2.284007e+08\n",
      "3                Code_du_Departement_01  2.284007e+08  2.284007e+08\n",
      "61               Code_du_Departement_58  2.284007e+08  2.284007e+08\n",
      "42               Code_du_Departement_39  2.284007e+08  2.284007e+08\n",
      "94               Code_du_Departement_91  2.284007e+08  2.284007e+08\n",
      "9                Code_du_Departement_07  2.284007e+08  2.284007e+08\n",
      "100             Code_du_Departement_972  2.284007e+08  2.284007e+08\n",
      "53               Code_du_Departement_50  2.284007e+08  2.284007e+08\n",
      "90               Code_du_Departement_87  2.284007e+08  2.284007e+08\n",
      "84               Code_du_Departement_81  2.284007e+08  2.284007e+08\n",
      "33               Code_du_Departement_30  2.284007e+08  2.284007e+08\n",
      "19               Code_du_Departement_17  2.284006e+08  2.284006e+08\n",
      "39               Code_du_Departement_36  2.284006e+08  2.284006e+08\n",
      "12               Code_du_Departement_10  2.284006e+08  2.284006e+08\n",
      "77               Code_du_Departement_74  2.284006e+08  2.284006e+08\n",
      "69               Code_du_Departement_66  2.284006e+08  2.284006e+08\n",
      "67               Code_du_Departement_64  2.284006e+08  2.284006e+08\n",
      "40               Code_du_Departement_37  2.284006e+08  2.284006e+08\n",
      "95               Code_du_Departement_92  2.284006e+08  2.284006e+08\n",
      "16               Code_du_Departement_14  2.284006e+08  2.284006e+08\n",
      "25               Code_du_Departement_24  2.284006e+08  2.284006e+08\n",
      "75               Code_du_Departement_72  2.284006e+08  2.284006e+08\n",
      "43               Code_du_Departement_40  2.284006e+08  2.284006e+08\n",
      "14               Code_du_Departement_12  2.284006e+08  2.284006e+08\n",
      "44               Code_du_Departement_41  2.284006e+08  2.284006e+08\n",
      "47               Code_du_Departement_44  2.284006e+08  2.284006e+08\n",
      "6                Code_du_Departement_04  2.284006e+08  2.284006e+08\n",
      "48               Code_du_Departement_45  2.284006e+08  2.284006e+08\n",
      "64               Code_du_Departement_61  2.284006e+08  2.284006e+08\n",
      "51               Code_du_Departement_48  2.284006e+08  2.284006e+08\n",
      "52               Code_du_Departement_49  2.284006e+08  2.284006e+08\n",
      "73               Code_du_Departement_70  2.284006e+08  2.284006e+08\n",
      "56               Code_du_Departement_53  2.284006e+08  2.284006e+08\n",
      "82               Code_du_Departement_79  2.284006e+08  2.284006e+08\n",
      "21               Code_du_Departement_19  2.284006e+08  2.284006e+08\n",
      "26               Code_du_Departement_25  2.284006e+08  2.284006e+08\n",
      "18               Code_du_Departement_16  2.284006e+08  2.284006e+08\n",
      "85               Code_du_Departement_82  2.284006e+08  2.284006e+08\n",
      "50               Code_du_Departement_47  2.284006e+08  2.284006e+08\n",
      "31               Code_du_Departement_2A  2.284006e+08  2.284006e+08\n",
      "59               Code_du_Departement_56  2.284006e+08  2.284006e+08\n",
      "46               Code_du_Departement_43  2.284006e+08  2.284006e+08\n",
      "17               Code_du_Departement_15  2.284006e+08  2.284006e+08\n",
      "35               Code_du_Departement_32  2.284006e+08  2.284006e+08\n",
      "7                Code_du_Departement_05  2.284006e+08  2.284006e+08\n",
      "49               Code_du_Departement_46  2.284006e+08  2.284006e+08\n",
      "37               Code_du_Departement_34  2.284006e+08  2.284006e+08\n",
      "89               Code_du_Departement_86  2.284006e+08  2.284006e+08\n",
      "11               Code_du_Departement_09  2.284006e+08  2.284006e+08\n",
      "68               Code_du_Departement_65  2.284006e+08  2.284006e+08\n",
      "30               Code_du_Departement_29  2.284006e+08  2.284006e+08\n",
      "32               Code_du_Departement_2B  2.284006e+08  2.284006e+08\n",
      "38               Code_du_Departement_35  2.284006e+08  2.284006e+08\n",
      "24               Code_du_Departement_23  2.284006e+08  2.284006e+08\n",
      "29               Code_du_Departement_28  2.284006e+08  2.284006e+08\n",
      "88               Code_du_Departement_85  2.284006e+08  2.284006e+08\n",
      "23               Code_du_Departement_22  2.284006e+08  2.284006e+08\n",
      "104             Code_du_Departement_978  2.284006e+08  2.284006e+08\n",
      "36               Code_du_Departement_33  2.284006e+08  2.284006e+08\n",
      "103             Code_du_Departement_976  2.284005e+08  2.284005e+08\n",
      "102             Code_du_Departement_974  2.284005e+08  2.284005e+08\n",
      "34               Code_du_Departement_31  2.284004e+08  2.284004e+08\n",
      "101             Code_du_Departement_973  2.284004e+08  2.284004e+08\n",
      "15               Code_du_Departement_13  2.284003e+08  2.284003e+08\n",
      "1    Nb_actuellement_en_soins_intensifs  2.209095e+00  2.209095e+00\n",
      "0          Nb_actuellement_hospitalises -2.550077e-01  2.550077e-01\n",
      "2               Total_retour_a_domicile  1.727804e-01  1.727804e-01\n",
      "  > Provide stats related to rate of patients in critical car over hospitalization :\n",
      "    - Computed values on  13.81 % of the full dataset\n",
      "    - maximum: 80.00 \n",
      "    - average: 12.23 \n",
      "    - median : 10.13\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13736\\2270176965.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    737\u001b[0m                   'rate of male that died due to covid-19':['Sexe_Homme', 'Total_Deces']}\n\u001b[0;32m    738\u001b[0m         \u001b[0mmin_sample_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m         \u001b[0mstats_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_selected_rate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_sample_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m         '''\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13736\\2270176965.py\u001b[0m in \u001b[0;36mget_selected_rate\u001b[1;34m(debug, df, inputs, min_sample_size)\u001b[0m\n\u001b[0;32m    663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfound_xfr_rate_float\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_ratio_of_result\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfound_ratio_of_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# accuracy to find the result = 0.1%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m                 \u001b[0mtgt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_ratio_of_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_xfr_rate_float\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 665\u001b[1;33m                 \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;31m# compile results in a dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    666\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'    - '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"{:.2f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_ratio_of_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'of the transfer rate values are equal or below'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"{:.2f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_xfr_rate_float\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Display results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import csv\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix\n",
    "\n",
    "__version__ = \"20220927\"\n",
    "__author__ = \"L.COSTA (ATR)\"\n",
    "\n",
    "\n",
    "def get_inputs(debug:bool, full_dataset:bool) -> dict:\n",
    "    # purpose\n",
    "    # input:\n",
    "    #   debug            : True (display additional information for debug) or False (basic display)\n",
    "    #   full_dataset     : True/False status for using the full dataset or only a selection of variables\n",
    "    # ouput:\n",
    "    #   return ...       : dictionary with 'filepath' str, 'response' string as target variable for modeling, 'variables' as input variables for modeling \n",
    "    filepath = None    # full path of the input data file\n",
    "    response = None    # name of the response variable (i.e. target for the model)\n",
    "    useful_vars = None # list of names of all selected (useful) variables as inputs for modeling\n",
    "\n",
    "\n",
    "    filepath = \"C:\\\\Users\\\\to202835\\\\Documents\\\\exploitation\\\\formation\\\\db_covid19\\\\donnees-hospitalieres-covid-19-dep-france.csv\"\n",
    "\n",
    "    response = 'Total_Deces' # My target for the modeling\n",
    "\n",
    "    if not full_dataset:\n",
    "        # File's variables that I can use for the project\n",
    "        useful_vars = ['Code_du_Departement','Date','Nb_actuellement_hospitalises','Nb_actuellement_en_soins_intensifs','Total_retour_a_domicile','Total_Deces','Code_region','Code_ISO_3166_de_la_zone','Nom_region','Nom_departement','Sexe','geo_point_2d','HospConv','SSR_USLD','autres','Nb_Quotidien_Admis_Hospitalisation','Nb_Quotidien_Admis_Reanimation','Nb_Quotidien_Deces','Nb_Quotidien_Retour_a_Domicile']\n",
    "        #   r2_scores_train: 0.9795083631226138\n",
    "        #   r2_scores_test: 0.9788894211053454\n",
    "\n",
    "        # Reduced set of variables considered for modeling\n",
    "\n",
    "        useful_vars = ['Code_region', 'Code_du_Departement','Nb_actuellement_hospitalises','Nb_actuellement_en_soins_intensifs','Total_retour_a_domicile','Total_Deces','Sexe','HospConv','Nb_Quotidien_Admis_Hospitalisation','Nb_Quotidien_Admis_Reanimation','Nb_Quotidien_Deces','Nb_Quotidien_Retour_a_Domicile']\n",
    "        #   r2_scores_train: 0.9761588302088899\n",
    "        #   r2_scores_test: 0.9755838001304158\n",
    "\n",
    "        useful_vars = ['Code_region', 'Code_du_Departement','Nb_actuellement_hospitalises','Nb_actuellement_en_soins_intensifs','Total_retour_a_domicile','Total_Deces','Sexe','HospConv'] # out ,'Nb_Quotidien_Admis_Hospitalisation','Nb_Quotidien_Admis_Reanimation','Nb_Quotidien_Deces','Nb_Quotidien_Retour_a_Domicile'\n",
    "        #   r2_scores_train: 0.9761588302088899\n",
    "        #   r2_scores_test: 0.9755838001304158\n",
    "\n",
    "        useful_vars = ['Code_region', 'Code_du_Departement','Total_retour_a_domicile','Total_Deces','Sexe','HospConv'] # out ,'Nb_Quotidien_Admis_Hospitalisation','Nb_Quotidien_Admis_Reanimation','Nb_Quotidien_Deces','Nb_Quotidien_Retour_a_Domicile','Nb_actuellement_hospitalises','Nb_actuellement_en_soins_intensifs'\n",
    "        #   r2_scores_train: 0.9739281412849445\n",
    "        #   r2_scores_test: 0.9734957171394152\n",
    "\n",
    "        useful_vars = ['Code_region', 'Code_du_Departement','Total_retour_a_domicile','Total_Deces','Sexe'] # out ,'Nb_Quotidien_Admis_Hospitalisation','Nb_Quotidien_Admis_Reanimation','Nb_Quotidien_Deces','Nb_Quotidien_Retour_a_Domicile','Nb_actuellement_hospitalises','Nb_actuellement_en_soins_intensifs','HospConv'\n",
    "        #   r2_scores_train: 0.9739281412849445\n",
    "        #   r2_scores_test: 0.9734957171394152\n",
    "\n",
    "        useful_vars = ['Code_region', 'Code_du_Departement', 'Nb_actuellement_hospitalises','Nb_actuellement_en_soins_intensifs', 'Total_retour_a_domicile','Total_Deces','Sexe'] # out ,'Nb_Quotidien_Admis_Hospitalisation','Nb_Quotidien_Admis_Reanimation','Nb_Quotidien_Deces','Nb_Quotidien_Retour_a_Domicile','Nb_actuellement_hospitalises','Nb_actuellement_en_soins_intensifs','HospConv'\n",
    "        #   r2_scores_train: 0.9761588302088899\n",
    "        #   r2_scores_test: 0.9755838001304158\n",
    "        # We keep this settings\n",
    "\n",
    "        useful_vars = ['Nb_actuellement_hospitalises','Nb_actuellement_en_soins_intensifs', 'Total_retour_a_domicile','Total_Deces','Sexe'] # out ,'Nb_Quotidien_Admis_Hospitalisation','Nb_Quotidien_Admis_Reanimation','Nb_Quotidien_Deces','Nb_Quotidien_Retour_a_Domicile','Nb_actuellement_hospitalises','Nb_actuellement_en_soins_intensifs','HospConv', 'Code_region', 'Code_du_Departement'\n",
    "        #   r2_scores_train: 0.9523048607117208\n",
    "        #   r2_scores_test: 0.9511261411557373\n",
    "\n",
    "        useful_vars = ['Code_du_Departement', 'Nb_actuellement_hospitalises','Nb_actuellement_en_soins_intensifs', 'Total_retour_a_domicile','Total_Deces','Sexe'] # out ,'Nb_Quotidien_Admis_Hospitalisation','Nb_Quotidien_Admis_Reanimation','Nb_Quotidien_Deces','Nb_Quotidien_Retour_a_Domicile','Nb_actuellement_hospitalises','Nb_actuellement_en_soins_intensifs','HospConv', 'Code_region'\n",
    "        #   r2_scores_train: 0.9761588302089482\n",
    "        #   r2_scores_test: 0.9755838001373758\n",
    "        # ***** We keep this settings, optimizing quantity of variables to establish the model ****\n",
    "    else:\n",
    "        # **** To activate for answering question related to the most missing data ****\n",
    "        useful_vars = ['Code_du_Departement','Date','Nb_actuellement_hospitalises','Nb_actuellement_en_soins_intensifs','Total_retour_a_domicile','Total_Deces','Code_region','Code_ISO_3166_de_la_zone','Nom_region','Nom_departement','Sexe','geo_point_2d','HospConv','SSR_USLD','autres','Nb_Quotidien_Admis_Hospitalisation','Nb_Quotidien_Admis_Reanimation','Nb_Quotidien_Deces','Nb_Quotidien_Retour_a_Domicile']\n",
    "\n",
    "    del debug, full_dataset\n",
    "    return {'filepath': filepath, 'response': response, 'variables': useful_vars}\n",
    "\n",
    "\n",
    "def read_file_data(debug:bool, data_reading_inputs:dict) -> object:\n",
    "    # purpose: Read and get data from one file\n",
    "    # input:\n",
    "    #   debug               : True (display additional information for debug) or False (basic display)\n",
    "    #   data_reading_inputs : .... pandas dataframe path is the path of the folder that contains all raw data csv files\n",
    "    # ouput:\n",
    "    #   df_list             : List of all df read on csv files content, with an additional column reporting the year of the results\n",
    "    df = None\n",
    "\n",
    "    # sub-inputs variables:\n",
    "    filepath = data_reading_inputs['filepath']\n",
    "    variables = data_reading_inputs['variables']\n",
    "    response = data_reading_inputs['response']\n",
    "    avail_vars = None\n",
    "\n",
    "    print (\"> Read data\")\n",
    "    if filepath is None:\n",
    "        print (\"  - No file selected\")\n",
    "    else: # Read the file\n",
    "        print (\"  - Input file:\", filepath)\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, sep=';', encoding='latin-1', low_memory=False) # Read file with encoding latin-1 due to occurrence of non-utf8\n",
    "                                                                                      # Add a low_memory=False to avoid error\n",
    "        except:\n",
    "            print(\"  CAUTION: Unable to read the file\")\n",
    "\n",
    "        if df.shape[1] == 0: # is df empty ? case of no column, whatever rows index\n",
    "            df = None # we reset df to avoid further abnormal use\n",
    "\n",
    "        if df is not None: # df has a content ...\n",
    "            if variables is None: # ... but no variables to look for were defined\n",
    "                print('  CAUTION: No useful variables pre-defined')\n",
    "                variables = df.columns # when no useful variables are pre-defined, we call all variables available in the file\n",
    "                if debug: print(\"variables: \", variables)\n",
    "                data_reading_inputs['variables'] = variables\n",
    "                avail_vars = df.columns\n",
    "                print('  - ', len(variables), ' variables found in the file will be considered')\n",
    "            else: # ... variables were defined to look for into the df\n",
    "                # Check all useful variables are available in the raw data\n",
    "                avail_vars = [] # variables pre-defined which are actually available within df\n",
    "                for var in variables:\n",
    "                    if var in df.columns:\n",
    "                        avail_vars.append(var)\n",
    "\n",
    "            if len(variables) != len(avail_vars):\n",
    "                    print('  CAUTION: Unable to get all usefull variables') \n",
    "            else:\n",
    "                # Get only useful declared variables\n",
    "                df_use = df[variables] # Keep only useful vars into df\n",
    "                df = df_use.copy(deep=True) # copy result into df (df is replaced only with these vars)\n",
    "                del df_use\n",
    "\n",
    "            if response not in df.columns:\n",
    "                print(\"  CAUTION: Response variable '\", response, \"' is not available in df\")\n",
    "                df = None\n",
    "\n",
    "    if debug and (df is not None):\n",
    "        print('  - df shape  :', df.shape)\n",
    "        print('  - df columns:', df.columns)\n",
    "    del debug, filepath, variables, avail_vars\n",
    "    return df, data_reading_inputs\n",
    "\n",
    "\n",
    "def get_df_size_delta(debug, shape_1, shape_2) -> list:\n",
    "    # purpose : compute and display difference of size between two df \n",
    "    # input:\n",
    "    #   debug : True (display additional information for debug) or False (basic display)\n",
    "    #   shape_1 : shape of first dataframe\n",
    "    #   shape_2 : shape of second dataframe\n",
    "    # ouput:\n",
    "    #   list of [difference of number of rows: 2-1, difference of number of columns: 2-1 ]\n",
    "\n",
    "    # sub-variables\n",
    "    delta_row = shape_1[0] - shape_2[0]\n",
    "    delta_col = shape_1[1] - shape_2[1]\n",
    "\n",
    "    if debug: \n",
    "        if delta_row != 0:\n",
    "            if delta_row > 0: signe = \"+\"\n",
    "            else: signe = \"\"\n",
    "            print('  - Change of row nb :', signe, delta_row)\n",
    "        if delta_col != 0:\n",
    "            if delta_col > 0: signe = \"+\"\n",
    "            else: signe = \"\"\n",
    "            print('  - Change of col. nb:', signe, delta_col)\n",
    "\n",
    "    return [delta_row, delta_col]\n",
    "\n",
    "\n",
    "\n",
    "def manage_dummy_df(debug, df: object, data_reading_inputs:dict) -> object:\n",
    "    # purpose : Dummy categorical variables within the dataframe\n",
    "    # input:\n",
    "    #   debug : True (display additional information for debug) or False (basic display)\n",
    "    #   df    : pandas dataframe with categorical variables that i'd like to dummy\n",
    "    # output:\n",
    "    #   df : dataframe with non categorical (copy of df) and categorical data that has the following characteristics:\n",
    "    #           1. contains all columns that were not specified as categorical\n",
    "    #           2. removes all the original columns in cat_df\n",
    "    #           3. dummy columns for each of the categorical columns in cat_df\n",
    "    #           4. if dummy_na is True - it also contains dummy columns for the NaN values\n",
    "    #           5. Use a prefix of the column name with an underscore (_) for separating \n",
    "\n",
    "    # sub-variables\n",
    "    cat_df = None\n",
    "    df_col, nb_add_var, df_samp, df_samp_col, drop_var, add_var, final_df_size, init_df_size, delta = None, None, None, None, None, None, None, None, None\n",
    "    response = data_reading_inputs['response']    \n",
    "\n",
    "    if df is not None:\n",
    "        init_df_size = df.shape\n",
    "\n",
    "        print (\"> Dummy cat. data\")\n",
    "        try:\n",
    "            if debug:\n",
    "                types_occur = df.dtypes.value_counts()\n",
    "                print(\"  - types_occur:\\n\", types_occur)\n",
    "            cat_df = df.select_dtypes(include=['object']) # Return a subset of categorical df's columns, in addition of non-categorical columns\n",
    "           \n",
    "            # Identify cat / non-cat columns\n",
    "            list_df = df.columns\n",
    "            list_df_cat = cat_df.columns\n",
    "            list_df_non_cat = []\n",
    "            for var in list_df:\n",
    "                if var not in list_df_cat:\n",
    "                    list_df_non_cat.append(var)\n",
    "            if debug:\n",
    "                print(\"  - list cat    :\", list_df_cat)\n",
    "                print(\"  - list non cat:\", list_df_non_cat)\n",
    "            del list_df, list_df_cat, list_df_non_cat\n",
    "        except:\n",
    "            print(\"  CAUTION: Unable to find categorical var in df\") \n",
    "            cat_df = pd.DataFrame([])\n",
    "        \n",
    "        if debug:\n",
    "            print('  - Nb of categorical data detected:', cat_df.shape[1])\n",
    "\n",
    "        if cat_df.shape[1] == 0: # is df empty ? case of no column, whatever rows index\n",
    "            print(\"  CAUTION: No categorical data found in raw data\") \n",
    "        else:            \n",
    "            for var in cat_df: # Run along the categorical data columns\n",
    "                if var != response: # We exclude the response variable of this process\n",
    "                    if debug:\n",
    "                        print(\"  - var '\", var, \"'\")\n",
    "                    try:\n",
    "                        df = pd.concat([df.drop(var, axis=1), pd.get_dummies(df[var], prefix=var, prefix_sep='_', drop_first=False)], axis=1)\n",
    "                        '''\n",
    "                        Initially, i've limited the number of new dummy variables to create into df to 10 but it was not enough to get a meomry error at this step.\n",
    "                        such finally, I reduce the file's content to only 3 years instead of a century\n",
    "                        '''\n",
    "                    except:\n",
    "                        print(\"    CAUTION: Unable to concat cat. var. '\", var, \"'\")\n",
    "                        continue\n",
    "\n",
    "        if debug and (df is not None):\n",
    "            final_df_size = df.shape\n",
    "            print('  - df shape      :', df.shape)\n",
    "            print('  - df columns    :', df.columns)\n",
    "            delta = get_df_size_delta(debug, init_df_size, final_df_size)\n",
    "        \n",
    "    del debug, cat_df, df_samp, df_col, nb_add_var, df_samp_col, drop_var, add_var, final_df_size, init_df_size, delta\n",
    "    return df\n",
    "\n",
    "\n",
    "def manage_invalid_data(debug, df: object) -> object:\n",
    "    # purpose : Manage variable with infinite values; remove the column\n",
    "    # input:\n",
    "    #   debug : True (display additional information for debug) or False (basic display)\n",
    "    #   df    : pandas dataframe with categorical variables that i'd like to dummy\n",
    "    # output:\n",
    "    #   df : df with infinite values replaced by nan\n",
    "\n",
    "    # sub-variables\n",
    "    a, isfinite, array = None, None, None # a is content of a column for one given variable; column with finite status for related column value; \n",
    "\n",
    "    if df is not None:\n",
    "\n",
    "        print (\"  > Manage invalid data\")\n",
    "        if debug:\n",
    "            print(\"    - df.columns:\", df.columns)\n",
    "\n",
    "        # Manage infinite data\n",
    "        if debug:\n",
    "            print (\"    > Manage infinite data\")\n",
    "\n",
    "        for var in df.columns:\n",
    "            a = df[var]\n",
    "            if a.dtypes != object:\n",
    "                isfinite = np.isfinite(a)\n",
    "                if not np.all(isfinite):\n",
    "                    if debug:\n",
    "                        print(\"      - \", var, \"contains infinite values\")\n",
    "                    try:\n",
    "                        if a.dtypes == 'float64':\n",
    "                            df = df.drop(columns=[var]) # Drop/Remove var from df\n",
    "                            if debug: print(\"        - float64 variable removed\") # For the time being, no other solution that removed the column ; other did not work\n",
    "                        else:\n",
    "                            array = np.where(not(isfinite), np.nan, a) # replace infinite value by nan\n",
    "                            df[var] = pd.Series(c_array)\n",
    "                            if debug: print(\"        - Infinite values (other than float64) replaced by nan\")\n",
    "                    except:\n",
    "                        print(\"      CAUTION: Unable to replace infinite values on\", var, ' (', a.dtypes, ')')\n",
    "                        continue\n",
    "\n",
    "        # Manage missing numeric value; Fill numeric columns with the mean\n",
    "        if debug:\n",
    "            print (\"    > Manage missing numerical data\")\n",
    "\n",
    "        num_vars = df.select_dtypes(include=['float', 'int']).columns\n",
    "        if len(num_vars) > 0:\n",
    "            for col in num_vars:\n",
    "                if df[col].isnull().values.any(): # When variable contains nan to replace by mean value, then ...\n",
    "                    try:\n",
    "                        if debug: print(\"      - Add mean value on: \", col)\n",
    "                        df[col].fillna((df[col].mean()), inplace=True) # ... Replace nan by mean value of the var\n",
    "                    except:\n",
    "                        if debug: print(\"      CAUTION: Unable to fill the mean for var '\", col, \"'\") \n",
    "        else:\n",
    "            if debug: print (\"      - No float/int variable found in df\")\n",
    "\n",
    "    del a, isfinite, array, num_vars \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_df(debug, df: object, data_reading_inputs: dict) -> object: # object is df\n",
    "    # purpose : remove infinite values \n",
    "    # input:\n",
    "    #   debug : True (display additional information for debug) or False (basic display)\n",
    "    #   df    : pandas dataframe  dataframe\n",
    "    # ouput:\n",
    "    #   df : input dataframe after management of missing and infinite data\n",
    "    \n",
    "    # sub-variables\n",
    "    df_r = None\n",
    "    delta, df_size_init = None, None\n",
    "    response = data_reading_inputs['response']\n",
    "\n",
    "    print(\"> Clean df\") # Manage missing data\n",
    "    if debug:\n",
    "        print(\"  > Impute data: Remove all empty row/column\")\n",
    "        print(\"    - response:\", response)\n",
    "        print(\"    - df.columns:\", df.columns)\n",
    "    try:\n",
    "        if df is not None:\n",
    "            df_size_init = df.shape\n",
    "            df_r = df.dropna(axis=0, how='all') # Manage rows with none data ; Drop rows with all missing values\n",
    "            df = df_r.dropna(axis=1, how='all') # Manage columns with none data ; Drop columns with all missing values\n",
    "            delta = get_df_size_delta(debug, df_size_init, df.shape) # Result of the clean ops\n",
    "    except:\n",
    "        print(\"  CAUTION: Unable to remove empty row/column\")  \n",
    "    if response not in df.columns:\n",
    "        print(\"  CAUTION: the Response variable '\", response, \"' is not available in the df\")\n",
    "        df = None\n",
    "\n",
    "    # Drop rows with missing response values\n",
    "    if debug:\n",
    "        print(\"  > Remove rows with missing value from the response column\")\n",
    "        print(\"    - response:\", response)\n",
    "        print(\"    - df.columns:\", df.columns)\n",
    "    try:\n",
    "        if df is not None:\n",
    "            df_size_init = df.shape\n",
    "            df = df.dropna(subset=[response]) # remove rows (axis=0 by default) with missing value in the column label (subset=) pred_name\n",
    "            delta = get_df_size_delta(debug, df_size_init, df.shape) # Result of the clean ops\n",
    "    except:\n",
    "        print(\"  CAUTION: Unable to remove missing values from the response column\")        \n",
    "\n",
    "    # Manage invalid values\n",
    "    df_size_init = df.shape\n",
    "    df = manage_invalid_data(debug, df)\n",
    "    delta = get_df_size_delta(debug, df_size_init, df.shape) # Result of the clean ops\n",
    "\n",
    "    del debug, data_reading_inputs, df_r, response, delta, df_size_init\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_X_y(debug, df: object, data_reading_inputs: dict):\n",
    "    # purpose : split df into exploratory X data and response y data\n",
    "    # input:\n",
    "    #   debug    : True (display additional information for debug) or False (basic display)\n",
    "    #   df       : pandas dataframe \n",
    "    #   response : response column's name\n",
    "    # ouput:\n",
    "    #   X : A matrix holding all of the variables you want to consider when predicting the response\n",
    "    #   y : the corresponding response vector\n",
    "    X, y = None, None\n",
    "\n",
    "    # sub-variables\n",
    "    response = data_reading_inputs['response']\n",
    "\n",
    "    print(\"> Split data into X/y\")\n",
    "    if debug:\n",
    "        print(\"  - response:\", response)\n",
    "        print(\"  - df.columns:\", df.columns)\n",
    "\n",
    "    # Get the Response var\n",
    "    if debug: print (\"  > Get y\")\n",
    "    if response in df.columns:\n",
    "        try:\n",
    "            y = df[response] # Split into explanatory and response variables (1/2) : Get response variable\n",
    "            df = df.drop(columns=[response]) # Remove pred_name from df\n",
    "        except:\n",
    "            print(\"    CAUTION: Unable to get the response data in df\")\n",
    "            y = None\n",
    "    else:\n",
    "        print(\"    CAUTION: Unable to find the response in df\")\n",
    "        y = None\n",
    " \n",
    "    # Get the Exploratory vars\n",
    "    if debug: print (\"  > Get X\")\n",
    "    try:\n",
    "        X = df.copy(deep=True) # Split into explanatory and response variables (2/2) : Get the input variables i.e. at this level just a copy of df\n",
    "    except:\n",
    "        print(\"    CAUTION: Unable to get the exploratory variables (X)\")\n",
    "        X = None\n",
    "\n",
    "    if debug and (y is not None):\n",
    "        print(\"    - y shape:\", y.shape)\n",
    "    if debug and (X is not None):\n",
    "        print(\"    - X shape:\", X.shape)\n",
    "\n",
    "    del debug, df, data_reading_inputs, response\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def get_model(debug, X: object, y: object, test_ratio=.3):\n",
    "    # function: Give a prediction model according to its X/y_test/train inputs/outputs values\n",
    "    # input:\n",
    "    #   debug      : True (display additional information for debug) or False (basic display)\n",
    "    #   X          : explanatory variables object\n",
    "    #   y          : response variable object\n",
    "    #   test_ratio : proportion of the dataset to include in the test split, between 0.0 and 1.0; default value = 0.3\n",
    "    # output:\n",
    "    #   model : linear regression model object from sklearn\n",
    "    #   score : Merge of mean square error value between Train and Test data set according to the proposed model\n",
    "    #   list of X_train and y_train\n",
    "    #   list of X_test and y_test\n",
    "    model, score = None, 0\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None\n",
    "\n",
    "    # sub-variables\n",
    "    y_pred, acc_score_train, acc_score_test, r2_scores_train, r2_scores_test = None, None, None, None, None\n",
    "\n",
    "    if (X is not None) and (y is not None):\n",
    "\n",
    "        print(\"> Get model\")\n",
    "\n",
    "        # Split into train and test X/y data set to establish the model and score it\n",
    "        print(\"  > Split Train / Test\")\n",
    "        try:\n",
    "            if debug: print(\"    - X/y size:\", X.shape, y.shape)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=42) \n",
    "            if debug: print(\"    - Train X/y size:\", X_train.shape, y_train.shape)\n",
    "            if debug: print(\"    - Test  X/y size:\", X_test.shape, y_test.shape)\n",
    "            split = True\n",
    "        except:\n",
    "            print(\"  CAUTION: Unable to split X/y into train and test data set\")\n",
    "            split = False\n",
    "\n",
    "        # Work on dtype\n",
    "        print(\"  > Type of data\")\n",
    "\n",
    "        if debug: print(\"    - y:\", y.dtypes)\n",
    "        \n",
    "        recensed_type = {}\n",
    "        for var in X.columns:\n",
    "            tip = X[var].dtypes\n",
    "            if tip not in recensed_type:\n",
    "                recensed_type[tip] = 1\n",
    "            else:\n",
    "                recensed_type[tip] = recensed_type[tip] +1\n",
    "        if len(recensed_type) > 0:\n",
    "            for key, value in recensed_type.items():\n",
    "                if debug: print(\"    - X\", key, \" : x\", value)\n",
    "            if debug: print(\"    - Over\", X.shape[1], \" columns\")\n",
    "\n",
    "\n",
    "        # Establish model\n",
    "        print(\"  > Modeling\")      \n",
    "        #try:\n",
    "        if split:\n",
    "            if debug: print(\"    - Train X/y size:\", X_train.shape, y_train.shape)\n",
    "            # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict / https://www.codegrepper.com/code-examples/python/reg.predict+python\n",
    "            model = LinearRegression().fit(X_train, y_train) # Fit linear model ; further methods: https://scikit-learn.org/stable/modules/linear_model.html\n",
    "            # It does not work\n",
    "\n",
    "            # https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification\n",
    "            # and https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "            #clf = linear_model.Ridge(alpha=1.0)\n",
    "            #Ridge(alpha=1.0)\n",
    "            #model = clf.fit(X_train, y_train)\n",
    "            # It does not work too\n",
    "\n",
    "            # Get the model's score\n",
    "            mdl_score = model.score(X_train, y_train) # Return the coefficient of determination of the prediction.\n",
    "            if debug: print(\"    - Train mdl_score:\", mdl_score)\n",
    "            del mdl_score\n",
    "            if debug: print(\"    - Test  X/y size:\", X_test.shape, y_test.shape)\n",
    "            y_pred = model.predict(X_test)\n",
    "        else:\n",
    "            y_pred = None\n",
    "        del split\n",
    "\n",
    "        # Evaluate this model\n",
    "        if y_pred is not None:\n",
    "            if debug: print (\"    - Model found\")\n",
    "\n",
    "        # Get metrics with a model by Regression\n",
    "        print(\"  > Metrics\") \n",
    "        if y_pred is not None:\n",
    "            # Accuracy_score (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "            # Confusion matrix (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) not appropriate for my purpose\n",
    "            # Common pitfalls (https://scikit-learn.org/stable/common_pitfalls.html): mean_sqaured_error and r2_score\n",
    "\n",
    "            '''\n",
    "            # According to https://stackoverflow.com/questions/37367405/python-scikit-learn-cant-handle-mix-of-multiclass-and-continuous\n",
    "            # ... Accuracy score is only for classification problems\n",
    "            acc_score_train = accuracy_score(y_train, model.predict(X_train))\n",
    "            acc_score_test = accuracy_score(y_test, model.predict(X_test))\n",
    "            if debug: print(\"    - acc_score_train:\", acc_score_train)\n",
    "            if debug: print(\"    - acc_score_test:\", acc_score_test)\n",
    "            '''\n",
    "            # always according to https://stackoverflow.com/questions/37367405/python-scikit-learn-cant-handle-mix-of-multiclass-and-continuous\n",
    "            # ... For regression problems you can use: R2 Score, MSE (Mean Squared Error), RMSE (Root Mean Squared Error).\n",
    "            r2_scores_train = r2_score(y_train, model.predict(X_train))\n",
    "            r2_scores_test = r2_score(y_test, model.predict(X_test))\n",
    "            print(\"    - r2_scores_train:\", r2_scores_train)\n",
    "            print(\"    - r2_scores_test:\", r2_scores_test)\n",
    "\n",
    "            score = r2_scores_train\n",
    "\n",
    "    del debug, X, y, test_ratio, y_pred, acc_score_train, acc_score_test, r2_scores_train, r2_scores_test\n",
    "    return model, score, [X_train, y_train], [X_test, y_test]\n",
    "\n",
    "\n",
    "def coef_weights(debug, model, X_train) -> object: # object is a df:\n",
    "    # function: get model's coefficients\n",
    "    # input:\n",
    "    #   debug      : True (display additional information for debug) or False (basic display)\n",
    "    #   model      : model for which we are looking coefficients\n",
    "    #   X_train    : the training data\n",
    "    # output:\n",
    "    #   coefs_df   : dataframe with model's coefficients; that can be used to understand the most influential coefficients\n",
    "    #                in a linear model by providing the coefficient estimates along with the name of the variable attached to the coefficient.\n",
    "\n",
    "    coefs_df = pd.DataFrame()\n",
    "    coefs_df['est_int'] = X_train.columns\n",
    "    coefs_df['coefs'] = model.coef_ # get coefficients of the linear model \n",
    "    coefs_df['abs_coefs'] = np.abs(model.coef_)\n",
    "    coefs_df = coefs_df.sort_values('abs_coefs', ascending=False)\n",
    "\n",
    "    del debug, model, X_train\n",
    "    return coefs_df\n",
    "\n",
    "\n",
    "def df_logical_no(debug, df, column):\n",
    "    # function: add a logical no values of a given column in a df\n",
    "    # input:\n",
    "    #   debug      : True (display additional information for debug) or False (basic display)\n",
    "    #   df         : initial dataframe to modify\n",
    "    #   column     : name of the df's column to compute a logical no for\n",
    "    # output:\n",
    "    #   df         : df completed with the column of logical no of the given column\n",
    "    #   New_column_name : name of the new column\n",
    "\n",
    "    liste, r, no_b = None, None, None\n",
    "    if debug: print('   - LOGICAL NO:', column) \n",
    "    try:\n",
    "        liste = df[column].values.tolist()\n",
    "        for i, j in enumerate(liste):\n",
    "            if j is None:\n",
    "                liste[i] = -1\n",
    "        complement = np.add(liste, 1) \n",
    "        no_liste = np.where(complement > 1, False, np.where(complement == 0, None, True))\n",
    "\n",
    "        New_column_name = column + '_logical_no'\n",
    "        df[New_column_name] = pd.Series(no_liste)\n",
    "        if debug: print('     - df: -> no', column, '\\n')\n",
    "    except:\n",
    "        print(\"    - Unable to compute logical no of column '\" + column + \"'\")\n",
    "\n",
    "    del debug, column, liste, r, no_b\n",
    "    return df, New_column_name\n",
    "\n",
    "\n",
    "def divide_serieA_by_serieB(debug, df, col_names, new_col, min_qty_col_B) -> object: # object is a df:\n",
    "    # function: divide values of two columns from of df and add result into this df\n",
    "    # input:\n",
    "    #   debug      : True (display additional information for debug) or False (basic display)\n",
    "    #   df         : initial dataframe to modify\n",
    "    #   cols       : names of df's column for numerator of the division and denominator of the division\n",
    "    #   new_col    : name of new df's column added with result of the division\n",
    "    # min_qty_col_B: Minimum value to consider on denominator do perform the computation\n",
    "    # output:\n",
    "    #   df         : df completed with the column of logical no of the given column\n",
    " \n",
    "    division = []    \n",
    "    listeA = df[col_names[0]].values.tolist()\n",
    "    listeB = df[col_names[1]].values.tolist()\n",
    "    \n",
    "    for i, A in enumerate (listeA):\n",
    "        try:\n",
    "            if listeB[i] >= min_qty_col_B:\n",
    "                r = 100* listeA[i]/listeB[i] # given in percentage\n",
    "            else:\n",
    "                r = None\n",
    "            if (i<10) or (r >99.9):\n",
    "                if debug: print('i=',i, '-', listeA[i], '/', listeB[i], '=', r)\n",
    "        except:\n",
    "            r = None\n",
    "            continue\n",
    "        division.append(r)\n",
    "\n",
    "    df[new_col] = pd.Series(division) # Add the result of the operation\n",
    "    df = df.sort_values(new_col, ascending=False) # sorted df by the result\n",
    "\n",
    "    del debug, col_names, new_col, listeA, listeB, division, min_qty_col_B\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_value_at_target_quantil(debug, target_quantil: float, df_column_data: object, nb_valid_data=None):\n",
    "    # function: Find the quantil value closest to the target quantil of a df column dataset\n",
    "    # input:\n",
    "    #   debug           : True (display additional information for debug) or False (basic display)\n",
    "    #   target_quantil  : target quantil (%)\n",
    "    #   df_column_data  : dataset of a df's column\n",
    "    #   nb_valid_data   : (option) ; size of the valid data to consider in the column ; by default the full size of df_column_data is considered\n",
    "    # output:\n",
    "    #   found_value     : value of the df_column_data that reach the target_quantil of data at or below this value\n",
    "    #   found_quantil   : actual quantil value (normally close to the target_quantil) that reach the closest the target quantil\n",
    "    \n",
    "    found_value, found_quantil = 0, 0 # Primary variables declaration\n",
    "    delta, min_delta, moving_value, moving_value_float, moving_quantil = 100, 100, 0, 0, 0 # sub-variables declaration\n",
    "\n",
    "    if nb_valid_data is None:\n",
    "        nb_valid_data = len(df_column_data)\n",
    "\n",
    "    if debug: print('>>> target_quantil =', target_quantil) # for instance 99.7% 'target quantil'\n",
    "    for moving_value in range(0, 10000, 1): # we run the 'moving value' from 0 to 100% by 0.01%-step\n",
    "        moving_value_float = moving_value/100\n",
    "        moving_quantil = 100 * df_column_data[df_column_data <= moving_value_float].count() / nb_valid_data #  get 'moving quantil' of 'value' below the 'moving value'\n",
    "        delta = abs(target_quantil - moving_quantil) # we compare the 'moving quantil' to the 'target quantil' we are trying to reach\n",
    "        if debug: print('    for moving_value_float:', \"{:.3f}\".format(moving_value_float), '\\t we reach moving_quantil:', \"{:.3f}\".format(moving_quantil), ' => delta ', \"{:.3f}\".format(delta), 'vs min_delta:', \"{:.3f}\".format(min_delta))\n",
    "        if delta <= min_delta: # when the gap is minimum, it means we find value of the 'value' for which we reach the targeted 'target quantil'\n",
    "            min_delta = delta\n",
    "            found_value = moving_value_float\n",
    "            found_quantil = moving_quantil\n",
    "            if debug: print('    change of delta')\n",
    "    if debug: print('    value:', found_value)\n",
    "\n",
    "    del debug, target_quantil, df_column_data, delta, min_delta, moving_value, moving_value_float, moving_quantil\n",
    "    return found_quantil, found_value\n",
    "\n",
    "\n",
    "def get_selected_rate(debug, df, inputs, min_sample_size):\n",
    "    # function: on a selected variable of the dataframe, provide stats (max, average, median) and find value at three target quantil\n",
    "    # input:\n",
    "    #   debug         : True (display additional information for debug) or False (basic display)\n",
    "    #     df          : dataframe to cope with\n",
    "    #  inputs         : dictionnary with result's titles as keys and a list with df's column names for numerator and denominator as values = {title1: [col_N1, col_D1], title2: [...] ...}\n",
    "    # min_sample_size : Minimum value to consider on denominator do perform the computation\n",
    "    # output:\n",
    "    #   outputs       : dictionary with result's titles as keys and a sub-dictionary with target percentage and result values\n",
    "\n",
    "    outputs = {}\n",
    "    tgt_pct = [99.7, 95.4, 68.3] # According to Gaussian theory: %-values related to 3sigma, 2sigma, 1sigma\n",
    "    found_ratio_of_result, found_xfr_rate_float = None, None \n",
    "\n",
    "    for title in inputs:\n",
    "        print(\"  > Provide stats related to\", title, \":\")\n",
    "        outputs[title] = title\n",
    "        num_denum = inputs[title] # get column's title of [numerator; denominator]\n",
    "        result_name = entree.replace(\" \", \"_\")\n",
    "        df = divide_serieA_by_serieB(debug, df, num_denum, result_name, min_sample_size)\n",
    "        results = {}\n",
    "        column = df[result_name]\n",
    "        nb_valid_data = len(df) - column.isna().sum()\n",
    "        ratio_valid_result = 100 * column.isna().sum() / len(df) # Compute ratio of computed result 'result_name' over the full size of data\n",
    "        print('    - Computed values on ', \"{:.2f}\".format(ratio_valid_result), '%', 'of the full dataset')\n",
    "        maximum_value = column.max() # Compute max value of 'result_name'\n",
    "        median_value = column.median() # Compute median value of 'result_name'        \n",
    "        average_value = column.mean() # Compute average value of 'result_name'\n",
    "        print('    - maximum:', \"{:.2f}\".format(maximum_value), '\\n    - average:', \"{:.2f}\".format(average_value), '\\n    - median :', \"{:.2f}\".format(median_value))\n",
    "        for target_ratio_of_result in tgt_pct: # we are looking the value (%) of xfr for which we have thr (serie here-above) below this value (%).\n",
    "                                                 # for instance, we look for the threshold value of xfr rate for which 95.4% of the xfr rate values are below this threshold value\n",
    "            found_ratio_of_result, found_xfr_rate_float = find_value_at_target_quantil(debug, target_ratio_of_result, column, nb_valid_data)\n",
    "            if (found_xfr_rate_float != 0) and (abs(target_ratio_of_result - found_ratio_of_result)<= 0.1): # accuracy to find the result = 0.1%\n",
    "                #tgt, value = str(target_ratio_of_result), str(found_xfr_rate_float)\n",
    "                results[target_ratio_of_result] = found_xfr_rate_float # compile results in a dictionary\n",
    "                print('    - ', \"{:.2f}\".format(target_ratio_of_result), '%', 'of the transfer rate values are equal or below', \"{:.2f}\".format(found_xfr_rate_float), '%') # Display results\n",
    "        outputs[title] = results\n",
    "\n",
    "    del debug, df, inputs, min_sample_size, tgt_pct, found_ratio_of_result, found_xfr_rate_float, num_denum, result_name, nb_valid_data, ratio_valid_result, maximum_value, median_value, average_value\n",
    "    return outputs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print (10*\"-\", \"PROJECT #01 / START\", 10*\"-\", \"\\n\")\n",
    " \n",
    "    # Display rules\n",
    "    # no tab / no symbol: High level title\n",
    "    # '>' Action step, tab length according to sub-level\n",
    "    # '-' Result, tab lentgth according to the sub-level\n",
    "    # 'CAUTION': abnormal/unexpected behaviour/result\n",
    "\n",
    "    # High level variables of the script for dev\n",
    "    debug = False  # if True, display additional information along the run of the app\n",
    "    use_full_data_set = False # read and customize analysis according to the use of the whole dataset or only a selection to optimize modeling effort\n",
    "    coef_df, most_missing_cols, df_context, stats_result = None, None, None, None\n",
    "    \n",
    "    if debug:\n",
    "        print('INFO! Debug mode is active!')\n",
    "    if use_full_data_set:\n",
    "        print('INFO! Use full dataset and realize appropriate analysis')\n",
    "    else:\n",
    "        print('INFO! Use selected dataset and realize appropriate analysis')\n",
    "\n",
    "    # Select input data\n",
    "    data_reading_inputs = get_inputs(debug, use_full_data_set) # get information on inputs to read and work on\n",
    "\n",
    "    # Gather data\n",
    "    df, data_reading_inputs = read_file_data(debug, data_reading_inputs) # : read data file and get pandas df with only requested raw data\n",
    "    df_init_copy = df.copy(deep=True)\n",
    "\n",
    "    # Manage dummy data\n",
    "    df = manage_dummy_df(debug, df, data_reading_inputs) # replace cat column by related dummy columns\n",
    "\n",
    "    # Clean data\n",
    "    df = clean_df(debug, df, data_reading_inputs) # drop rows / columns with all missing values + replace infinite values by nan\n",
    "\n",
    "    # Split into Response y / Exploratory variables X\n",
    "    X, y = get_X_y(debug, df, data_reading_inputs)\n",
    "\n",
    "    # modeling\n",
    "    model, score, Xy_train, Xy_test = get_model(debug, X, y)\n",
    "\n",
    "\n",
    "    # Questions\n",
    "    print('> Processing for providing stats')\n",
    "\n",
    "    if use_full_data_set:\n",
    "        # Is there any parameters more missing than others ?\n",
    "        print(\"  > Define most missing data:\")\n",
    "        print(\"    Over\", len(df_init_copy), \"of data entries reported\")\n",
    "        print(\"    Find here-below the ratio of missing data by categories of data:\")\n",
    "        df_init_copy_num_missing_rate = 100 *  df_init_copy.isna().sum() / len(df_init_copy) # \"df_init_copy.isna().sum()\" can be replaced by \"df_init_copy.isnull().sum()\"\"\n",
    "        # we can have the same result with \"df_init_copy_num_missing_mean = df_init_copy.isna().mean().round(4) * 100\"\n",
    "        print(df_init_copy_num_missing_rate.sort_values(ascending=False))\n",
    "        # Conversely, we could have the opposite result, i.e. available data ratio with \"df_init_copy_num_rate = 100*df_init_copy.count() / len(df_init_copy)\"\"\n",
    "    \n",
    "    else:\n",
    "        # Which variables have the most effect on the model ?\n",
    "        print('  > Define impact of variables on the model:')\n",
    "        coef_df = coef_weights(debug, model, Xy_train[0])\n",
    "        pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "        print('    ', coef_df)\n",
    "\n",
    "\n",
    "        inputs = {'rate of patients in critical car over hospitalization': ['Nb_actuellement_en_soins_intensifs', 'Nb_actuellement_hospitalises'],\\\n",
    "                  'rate of female that died due to covid-19': ['Sexe_Femme', 'Total_Deces'],\\\n",
    "                  'rate of male that died due to covid-19':['Sexe_Homme', 'Total_Deces']}\n",
    "        min_sample_size = 10\n",
    "        stats_result = get_selected_rate(debug, df, inputs, min_sample_size)\n",
    "\n",
    "        '''\n",
    "        print(\"  > Provide stats related to rate of female that died due to covid-19:\")\n",
    "        result_name = 'ratios_mortalite_femme'\n",
    "        nb_min_actuellement_hospitalises = 10\n",
    "        df = divide_serieA_by_serieB(debug, df, 'Sexe_Femme', 'Total_Deces', result_name, nb_min_actuellement_hospitalises)\n",
    "\n",
    "        column = df[result_name]\n",
    "        nb_valid_data = len(df) - column.isna().sum()\n",
    "        ratio_valid_result = 100 * column.isna().sum() / len(df) # Compute ratio of computed result 'result_name' over the full size of data\n",
    "        print('    - Computed values on ', \"{:.2f}\".format(ratio_valid_result), '%', 'of the full dataset')\n",
    "        maximum_value = column.max() # Compute max value of 'result_name'\n",
    "        median_value = column.median() # Compute median value of 'result_name'        \n",
    "        average_value = column.mean() # Compute average value of 'result_name'\n",
    "        print('    - maximum:', \"{:.2f}\".format(maximum_value), '\\n    - average:', \"{:.2f}\".format(average_value), '\\n    - median :', \"{:.2f}\".format(median_value))\n",
    "        serie_thr = [99.7, 95.4, 68.3] # TBD ...Define thresholds to compute a ratio of input above these thresholds\n",
    "        for target_ratio_of_result in serie_thr: # we are looking the value (%) of xfr for which we have thr (serie here-above) below this value (%).\n",
    "                                                 # for instance, we look for the threshold value of xfr rate for which 40% of the xfr rate values are below this threshold value\n",
    "            found_ratio_of_result, found_xfr_rate_float = find_value_at_target_quantil(debug, target_ratio_of_result, column, nb_valid_data)\n",
    "            if (found_xfr_rate_float != 0) and (abs(target_ratio_of_result - found_ratio_of_result)<= 0.1):\n",
    "                print('    - ', \"{:.2f}\".format(target_ratio_of_result), '%', 'of the transfer rate values are equal or below', \"{:.2f}\".format(found_xfr_rate_float), '%') # Display results\n",
    " \n",
    "        print(\"  > Provide stats related to rate of male that died due to covid-19:\")\n",
    "        result_name = 'ratios_mortalite_homme'\n",
    "        nb_min_actuellement_hospitalises = 10\n",
    "        df = divide_serieA_by_serieB(debug, df, 'Sexe_Homme', 'Total_Deces', result_name, nb_min_actuellement_hospitalises)\n",
    "\n",
    "        column = df[result_name]\n",
    "        nb_valid_data = len(df) - column.isna().sum()\n",
    "        ratio_valid_result = 100 * column.isna().sum() / len(df) # Compute ratio of computed result 'result_name' over the full size of data\n",
    "        print('    - Computed values on ', \"{:.2f}\".format(ratio_valid_result), '%', 'of the full dataset')\n",
    "        maximum_value = column.max() # Compute max value of 'result_name'\n",
    "        median_value = column.median() # Compute median value of 'result_name'        \n",
    "        average_value = column.mean() # Compute average value of 'result_name'\n",
    "        print('    - maximum:', \"{:.2f}\".format(maximum_value), '\\n    - average:', \"{:.2f}\".format(average_value), '\\n    - median :', \"{:.2f}\".format(median_value))\n",
    "        serie_thr = [99.7, 95.4, 68.3] # TBD ...Define thresholds to compute a ratio of input above these thresholds\n",
    "        for target_ratio_of_result in serie_thr: # we are looking the value (%) of xfr for which we have thr (serie here-above) below this value (%).\n",
    "                                                 # for instance, we look for the threshold value of xfr rate for which 40% of the xfr rate values are below this threshold value\n",
    "            found_ratio_of_result, found_xfr_rate_float = find_value_at_target_quantil(debug, target_ratio_of_result, column, nb_valid_data)\n",
    "            if (found_xfr_rate_float != 0) and (abs(target_ratio_of_result - found_ratio_of_result)<= 0.1):\n",
    "                print('    - ', \"{:.2f}\".format(target_ratio_of_result), '%', 'of the transfer rate values are equal or below', \"{:.2f}\".format(found_xfr_rate_float), '%') # Display results\n",
    "        '''\n",
    "\n",
    "    del debug, use_full_data_set, data_reading_inputs, df, df_init_copy, coef_df, most_missing_cols, df_context, stats_result\n",
    "    print ('\\n' + 10*\"-\", \"PROJECT #01 /  END \", 10*\"-\", \"\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit (system)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
