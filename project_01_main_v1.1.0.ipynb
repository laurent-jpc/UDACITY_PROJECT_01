{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pycodestyle_magic extension is already loaded. To reload it, use:\n",
      "  %reload_ext pycodestyle_magic\n"
     ]
    }
   ],
   "source": [
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- UDACITY - DATASICENCE PROJECT #01 / START ----------\n",
      "\tversion: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "# %%pycodestyle  # --> raises \"27:1: W391 blank line at end of file\"\n",
    "# while no obvious blank line after the last code line!\n",
    "# Disabled because it inhibits effect of variables and print lines.\n",
    "'''\n",
    "it should be used in a different markdown that %load_est pycodestyle_magic\n",
    "otherwise, it does not work!\n",
    "I got the message \"UsageError: Line magic function `%%pycodestyle` not found.\"\n",
    "'''\n",
    "# High level variables of the script for dev\n",
    "__version__ = \"1.1.0\"\n",
    "__date__ = \"17-oct-2022\"\n",
    "__author__ = \"L.COSTA (ATR)\"\n",
    "\n",
    "__debug_script__ = False\n",
    "# if True, display more feedback to ease debugging\n",
    "\n",
    "# Display rules:\n",
    "# no tab / no symbol: High level title\n",
    "# '>' Action step, tab length according to sub-level\n",
    "# '-' Result, tab lentgth according to the sub-level\n",
    "# 'CAUTION': abnormal/unexpected behaviour/result\n",
    "\n",
    "print(10 * \"-\", \"UDACITY - DATASICENCE PROJECT #01 / START\", 10 * \"-\")\n",
    "print(\"\\tversion:\", __version__)\n",
    "\n",
    "if __debug_script__:\n",
    "    print('INFO! Debug mode is active!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Read data\n",
      "  - Input file: C:\\Users\\to202835\\Documents\\exploitation\\formation\\db_covid19\\donnees-hospitalieres-covid-19-dep-france.csv\n",
      "> Preliminary monitoring of raw data for raising missing data and so potential difficulty for hospitals to report some data.\n",
      "  Is there any parameters more missing than others over the 280600 of data entries reported by hospitals ?\n",
      "  - Ratio of missing data by categories of data:\n",
      "Nb_Quotidien_Retour_a_Domicile        66.593728\n",
      "Nb_Quotidien_Deces                    66.593728\n",
      "Nb_Quotidien_Admis_Reanimation        66.593728\n",
      "Nb_Quotidien_Admis_Hospitalisation    66.593728\n",
      "autres                                39.239130\n",
      "SSR_USLD                              39.239130\n",
      "HospConv                              39.239130\n",
      "geo_point_2d                           0.655738\n",
      "Nom_departement                        0.655738\n",
      "Nom_region                             0.655738\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# %%pycodestyle  # Disabled because it raises the error message\n",
    "# \"too many values to unpack (expected 3)\" ...\n",
    "# \"do not subtract 1 for line for %%pycodestyle, inc pre py3.6 string\"\n",
    "# According to\n",
    "# \"https://stackoverflow.com/questions/61230004/\n",
    "#  ho-to-fix-valueerror-too-many-values-to-unpack-expected-3\"\n",
    "#  that seems to not to be linked to a mistaken code.\n",
    "\n",
    "# PRE-ANALYSIS OF THE RAW DATASET\n",
    "\n",
    "# Before using the dataset for usual 'data science' processing,\n",
    "# we gonna scan its content and raise data with the most missing data.\n",
    "# It could be help for selecting most relevant data for furrther modeling.\n",
    "# What are the most missing data provided by hospitals about Covid-19\n",
    "#  patients ?\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_input_filepath() -> str:\n",
    "    '''\n",
    "    returns the path of the selected csv data source file\n",
    "\n",
    "    input: None\n",
    "    ouput:\n",
    "           string of the data file's path (separator: back-slash)\n",
    "    '''\n",
    "    # function: Return input data file's path\n",
    "    directory = \"C:\\\\Users\\\\to202835\\\\Documents\"\n",
    "    path = \"exploitation\\\\formation\\\\db_covid19\"\n",
    "    filename = \"donnees-hospitalieres-covid-19-dep-france.csv\"\n",
    "\n",
    "    return \"\\\\\".join((directory, path, filename))\n",
    "\n",
    "\n",
    "def show_df_shape(df:object, description_name:str = ''):\n",
    "    '''\n",
    "    returns only a display of the shape and column's names of \n",
    "     a dataframe\n",
    "\n",
    "    input: \n",
    "           df  : dataframe \n",
    "           description_name : additional descriptive name to add at display\n",
    "    ouput: None\n",
    "    '''\n",
    "    # function: Display the shape and column's names of a dataframe\n",
    "\n",
    "    if (df is not None) and __debug_script__:\n",
    "        print('  -', description_name, 'df shape  :', df.shape)\n",
    "        print('  -', description_name, 'df columns:', df.columns)\n",
    "\n",
    "\n",
    "def read_csv_file(filepath:str) -> object:\n",
    "    '''\n",
    "    returns a dataframe of the data contained into the csv designated \n",
    "     by its file path\n",
    "\n",
    "    input:\n",
    "           filepath : Path of the csv data file that contains\n",
    "            all raw data\n",
    "    ouput:\n",
    "           df_list  : List of all df read on csv files content,\n",
    "            None otherwise\n",
    "    '''\n",
    "    # function: Read a csv data file and get its data into a pandas dataframe\n",
    "\n",
    "    # output\n",
    "    df = None\n",
    "\n",
    "    print (\"> Read data\")\n",
    "    if filepath is None:\n",
    "        print (\"  - No file selected\")\n",
    "    else:  # Read the file\n",
    "        print (\"  - Input file:\", filepath)\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, sep=';', encoding='latin-1', low_memory=False)\n",
    "            # Read file with encoding latin-1 due to occurrence of non-utf8\n",
    "            # Add a low_memory=False to avoid error\n",
    "        except:\n",
    "            print(\"  CAUTION: Unable to read the file\")\n",
    "\n",
    "         # is df empty ? case of no column, whatever rows index\n",
    "        if (df is not None) and (df.shape[1] == 0):\n",
    "            df = None  # we reset df to avoid further abnormal use\n",
    "        show_df_shape(df)\n",
    "    del filepath\n",
    "    return df\n",
    "\n",
    "\n",
    "# Gather data\n",
    "filepath = get_input_filepath()  # get path of the csv data file\n",
    "df = read_csv_file(filepath)     # read the data and get a pandas df\n",
    "\n",
    "# No assessment, no cleaning; I actually want to know the most missing data\n",
    "\n",
    "# Analyze\n",
    "if df is not None:\n",
    "    print(\"> Preliminary monitoring of raw data for raising missing data and so potential difficulty for hospitals to report some data.\")\n",
    "    print(\"  Is there any parameters more missing than others over the\", len(df), \"of data entries reported by hospitals ?\")\n",
    "    print(\"  - Ratio of missing data by categories of data:\")\n",
    "    df_num_missing_rate = 100 *  df.isna().sum() / len(df)\n",
    "    # \"df_init_copy.isna().sum()\" can be replaced by \"df_init_copy.isnull().sum()\"\"\n",
    "    #  we can have the same result with \"df_init_copy_num_missing_mean = df_init_copy.isna().mean().round(4) * 100\"\n",
    "    print(df_num_missing_rate.sort_values(ascending=False).head(10))\n",
    "    # Sort rates of missing values in the descending order to get interesting values first (for answering our question)\n",
    "    # Conversely, we could have the opposite result,\n",
    "    #  i.e. available data ratio with \"df_init_copy_num_rate = 100*df_init_copy.count() / len(df_init_copy)\"\"\n",
    "\n",
    "# Conclusion\n",
    "# We see that categories 'Nb_Quotidien_Retour_a_Domicile', 'Nb_Quotidien_Deces',\n",
    "#  'Nb_Quotidien_Admis_Reanimation' and 'Nb_Quotidien_Admis_Hospitalisation'\n",
    "#  have more than 66% of missing data; hostpitals can't or don't want fulfil these information.\n",
    "# Inaddition, categories 'autres', 'SSR_USLD' and 'HospConv' reach a bit less than 40%.\n",
    "# Maybe, I would have a better model using other remaining data which have less than 1% of missing data. \n",
    "\n",
    "# No modeling, no vizualization; I did not built a model at this stage; I'll do it after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Check availability of selected categories in the dataframe.\n"
     ]
    }
   ],
   "source": [
    "# %%pycodestyle  # Disabled because it raises an error message\n",
    "#  \"too many values to unpack (expected 3)\" linked to pycodestyle\n",
    "# See above.\n",
    "\n",
    "# Now that we identify on which categories we could built a model,\n",
    "# go ahead on building this model, unfolding the whole process\n",
    "\n",
    "# Running several times the process below allows definint the set\n",
    "#  of categories that provide a good model\n",
    "\n",
    "# GATHER\n",
    "\n",
    "\n",
    "def get_model_input() -> dict:\n",
    "    '''\n",
    "    returns a dictionary of pre-defined response category and a list\n",
    "     of variables categories\n",
    "\n",
    "    input: None\n",
    "    ouput:\n",
    "           dictionary of strings with 'response' as target\n",
    "            for modeling and 'variables' as input categories\n",
    "    '''\n",
    "    # function: Get inputs (model's category target & input cat.) for modeling\n",
    "\n",
    "    cat_response = None   # name of the response category as model's target\n",
    "    cat_variables = None  # list of categories selected as input for modeling\n",
    "\n",
    "    cat_response = 'Total_Deces'  # My target for the modeling\n",
    "\n",
    "    '''\n",
    "    # File's variables that I can use for the project\n",
    "    cat_variables = ['Code_du_Departement','Date',\n",
    "                     'Nb_actuellement_hospitalises',\n",
    "                     'Nb_actuellement_en_soins_intensifs',\n",
    "                     'Total_retour_a_domicile', 'Total_Deces', 'Code_region',\n",
    "                     'Code_ISO_3166_de_la_zone','Nom_region',\n",
    "                     'Nom_departement','Sexe','geo_point_2d','HospConv',\n",
    "                     'SSR_USLD', 'autres',\n",
    "                     'Nb_Quotidien_Admis_Hospitalisation',\n",
    "                     'Nb_Quotidien_Admis_Reanimation', 'Nb_Quotidien_Deces',\n",
    "                     'Nb_Quotidien_Retour_a_Domicile']\n",
    "    #   r2_scores_train: 0.9795083631226138\n",
    "    #   r2_scores_test: 0.9788894211053454\n",
    "\n",
    "    # Reduced set of variables considered for modeling\n",
    "\n",
    "    cat_variables = ['Code_region', 'Code_du_Departement',\n",
    "                     'Nb_actuellement_hospitalises',\n",
    "                     'Nb_actuellement_en_soins_intensifs',\n",
    "                     'Total_retour_a_domicile','Total_Deces','Sexe',\n",
    "                     'HospConv', 'Nb_Quotidien_Admis_Hospitalisation',\n",
    "                     'Nb_Quotidien_Admis_Reanimation', 'Nb_Quotidien_Deces',\n",
    "                     'Nb_Quotidien_Retour_a_Domicile']\n",
    "    #   r2_scores_train: 0.9761588302088899\n",
    "    #   r2_scores_test: 0.9755838001304158\n",
    "\n",
    "    cat_variables = ['Code_region', 'Code_du_Departement',\n",
    "                     'Nb_actuellement_hospitalises',\n",
    "                     'Nb_actuellement_en_soins_intensifs',\n",
    "                     'Total_retour_a_domicile','Total_Deces', 'Sexe',\n",
    "                     'HospConv']\n",
    "    # out ,'Nb_Quotidien_Admis_Hospitalisation',\n",
    "    #  'Nb_Quotidien_Admis_Reanimation','Nb_Quotidien_Deces',\n",
    "    #  'Nb_Quotidien_Retour_a_Domicile'\n",
    "    #   r2_scores_train: 0.9761588302088899\n",
    "    #   r2_scores_test: 0.9755838001304158\n",
    "\n",
    "    cat_variables = ['Code_region', 'Code_du_Departement',\n",
    "                     'Total_retour_a_domicile', 'Total_Deces','Sexe',\n",
    "                     'HospConv']\n",
    "    # out ,'Nb_Quotidien_Admis_Hospitalisation',\n",
    "    #  'Nb_Quotidien_Admis_Reanimation','Nb_Quotidien_Deces',\n",
    "    #  'Nb_Quotidien_Retour_a_Domicile', 'Nb_actuellement_hospitalises',\n",
    "    #  'Nb_actuellement_en_soins_intensifs'\n",
    "    #   r2_scores_train: 0.9739281412849445\n",
    "    #   r2_scores_test: 0.9734957171394152\n",
    "\n",
    "    cat_variables = ['Code_region', 'Code_du_Departement',\n",
    "                     'Total_retour_a_domicile', 'Total_Deces','Sexe']\n",
    "    # out ,'Nb_Quotidien_Admis_Hospitalisation',\n",
    "    #  'Nb_Quotidien_Admis_Reanimation','Nb_Quotidien_Deces',\n",
    "    #  'Nb_Quotidien_Retour_a_Domicile', 'Nb_actuellement_hospitalises',\n",
    "    #  'Nb_actuellement_en_soins_intensifs','HospConv'\n",
    "    #   r2_scores_train: 0.9739281412849445\n",
    "    #   r2_scores_test: 0.9734957171394152\n",
    "\n",
    "    cat_variables = ['Code_region', 'Code_du_Departement',\n",
    "                     'Nb_actuellement_hospitalises',\n",
    "                     'Nb_actuellement_en_soins_intensifs',\n",
    "                     'Total_retour_a_domicile','Total_Deces','Sexe']\n",
    "    # out ,'Nb_Quotidien_Admis_Hospitalisation',\n",
    "    #  'Nb_Quotidien_Admis_Reanimation','Nb_Quotidien_Deces',\n",
    "    #  'Nb_Quotidien_Retour_a_Domicile','Nb_actuellement_hospitalises',\n",
    "    #  'Nb_actuellement_en_soins_intensifs','HospConv'\n",
    "    #   r2_scores_train: 0.9761588302088899\n",
    "    #   r2_scores_test: 0.9755838001304158\n",
    "    # We keep this settings\n",
    "\n",
    "    cat_variables = ['Nb_actuellement_hospitalises',\n",
    "                     'Nb_actuellement_en_soins_intensifs',\n",
    "                     'Total_retour_a_domicile', 'Total_Deces','Sexe']\n",
    "    # out ,'Nb_Quotidien_Admis_Hospitalisation',\n",
    "    #  'Nb_Quotidien_Admis_Reanimation','Nb_Quotidien_Deces',\n",
    "    #  'Nb_Quotidien_Retour_a_Domicile', 'Nb_actuellement_hospitalises',\n",
    "    #  'Nb_actuellement_en_soins_intensifs','HospConv',\n",
    "    #  'Code_region', 'Code_du_Departement'\n",
    "    #   r2_scores_train: 0.9523048607117208\n",
    "    #   r2_scores_test: 0.9511261411557373\n",
    "    '''\n",
    "\n",
    "    cat_variables = ['Code_du_Departement', 'Nb_actuellement_hospitalises',\n",
    "                     'Nb_actuellement_en_soins_intensifs',\n",
    "                     'Total_retour_a_domicile', 'Total_Deces', 'Sexe']\n",
    "    # out ,'Nb_Quotidien_Admis_Hospitalisation',\n",
    "    # 'Nb_Quotidien_Admis_Reanimation','Nb_Quotidien_Deces',\n",
    "    # 'Nb_Quotidien_Retour_a_Domicile','Nb_actuellement_hospitalises',\n",
    "    # 'Nb_actuellement_en_soins_intensifs','HospConv', 'Code_region'\n",
    "    #   r2_scores_train: 0.9761588302089482\n",
    "    #   r2_scores_test: 0.9755838001373758\n",
    "    # I keep them, optimizing quantity of variables to establish the model\n",
    "\n",
    "    return {'response': cat_response, 'variables': cat_variables}\n",
    "\n",
    "\n",
    "def get_df_selection(df:object, categories_selection:dict):\n",
    "    '''\n",
    "    returns a dataframe only with selected categories and a dictionary\n",
    "     of the response and input categories, updated of what is actually\n",
    "     available within the initial dataframe.\n",
    "    \n",
    "     input:\n",
    "            df : dataframe with all categories\n",
    "            categories_selection: dict. of response & inputs cat.\n",
    "             to keep in df\n",
    "    ouput:\n",
    "            df : dataframe only with categories selection\n",
    "            categories_selection : Update (if needed) of the input dict.\n",
    "    '''\n",
    "    # function: get dataframe with selection of categories\n",
    "\n",
    "    # sub-inputs variables:\n",
    "    cat_variables = categories_selection['variables']\n",
    "    cat_response = categories_selection['response']\n",
    "    avail_variables, cat_variable, df_selection = None, None, None\n",
    "    df_complete = True\n",
    "\n",
    "    print (\"> Check availability of selected categories in the dataframe.\")\n",
    "    if df is not None:\n",
    "        avail_variables = []  # categories actualy present into the dataframe\n",
    "                              #  and considered for modeling\n",
    "        # If no category is selected, define a default sel.\n",
    "        if cat_variables is None:\n",
    "            print('  CAUTION: No useful variables pre-defined')            \n",
    "            # default selection = all cat. available in file\n",
    "            cat_variables = df.columns\n",
    "            if __debug_script__:\n",
    "                print(\"variables: \", cat_variables)\n",
    "            # Update dict. with new input\n",
    "            categories_selection['variables'] = cat_variables\n",
    "            avail_variables = df.columns\n",
    "            print('  - All', len(cat_variables), 'categories found & used in data file')\n",
    "        else:\n",
    "            # If categories of variables are selected\n",
    "            #  check all useful categories are available in the raw data\n",
    "            for cat_variable in cat_variables:\n",
    "                if cat_variable in df.columns:\n",
    "                    avail_variables.append(cat_variable)\n",
    "\n",
    "        if cat_response not in avail_variables:\n",
    "            print(\"  CAUTION: The model's response category was not found in dataframe\")\n",
    "            df_complete = False         \n",
    "\n",
    "        if len(cat_variables) != len(avail_variables):\n",
    "            # if we do not have the expected nb of categories\n",
    "\n",
    "            print('  CAUTION: Unable to get all usefull variables')\n",
    "            print('  - Found categories:', avail_variables)\n",
    "            df_complete = False      \n",
    "        \n",
    "        if df_complete:\n",
    "            # Get only useful declared categories\n",
    "            df_selection = df[cat_variables] # Keep only useful categories in a df\n",
    "            # df = df_use.copy(deep=True) # copy result into df\n",
    "            if __debug_script__:\n",
    "                print('  All', len(cat_variables),'selected categories available.')\n",
    "\n",
    "        show_df_shape(df_selection, 'Selection')\n",
    "\n",
    "    del cat_variables, cat_response, avail_variables, cat_variable, df_complete, df\n",
    "    return df_selection, categories_selection\n",
    "\n",
    "\n",
    "# Define a selection of categories to use for modeling\n",
    "categories_selection = get_model_input()\n",
    "\n",
    "# get dataframe of the selected categories \n",
    "df, categories_selection = get_df_selection(df, categories_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Dummy cat. data\n"
     ]
    }
   ],
   "source": [
    "# %%pycodestyle  # Disabled because it raises an error message\n",
    "#  \"too many values to unpack (expected 3)\" linked to pycodestyle\n",
    "# See above.\n",
    "\n",
    "# MANAGE DUMMY DATA\n",
    "\n",
    "# Now that we get the selected raw dataset, we gonna make some assessment\n",
    "# on this dataset\n",
    "\n",
    "\n",
    "def manage_dummy_df(df: object, categories_selection:dict) -> object:\n",
    "    '''\n",
    "    returns  the initial dataframe completed with all appropriate \n",
    "     new categories of dummy categories\n",
    "\n",
    "    input:\n",
    "           df : pandas dataframe with categorical var. that\n",
    "            i'd like to dummy\n",
    "    output:\n",
    "           df : dataframe with non categorical & categorical\n",
    "            data like:\n",
    "                1. contains all columns that were not specified\n",
    "                 as categorical\n",
    "                2. removes all the original columns in cat_df\n",
    "                3. dummy columns for each of the categorical\n",
    "                 columns in cat_df\n",
    "                4. if dummy_na is True, it contains dummy columns\n",
    "                 for NaN values\n",
    "                5. Use a prefix of column's name with underscore\n",
    "                 (_) as separator \n",
    "    '''\n",
    "    # purpose : Dummy categorical variables within the dataframe\n",
    "    # Actually, it concerns gender category and France's department \n",
    "    #  category; indeed, integer code number used for designated these\n",
    "    #  department are not recognized as figures.\n",
    "        \n",
    "    # sub-variables\n",
    "    cat_df, df_col, df_samp, df_samp_col = None, None, None, None\n",
    "    nb_add_var, drop_var, add_var = None, None, None\n",
    "    init_df_size, delta = None, None\n",
    "    response = categories_selection['response']    \n",
    "\n",
    "    if df is not None:\n",
    "\n",
    "        print (\"> Dummy cat. data\")\n",
    "\n",
    "        if __debug_script__ and (df is not None):\n",
    "            init_df_size = df.shape\n",
    "            show_df_shape(df, 'Before dummy')\n",
    "\n",
    "        try:\n",
    "            if __debug_script__:                \n",
    "                types_occur = df.dtypes.value_counts()\n",
    "                print(\"  - types_occur:\\n\", types_occur)\n",
    "            # Return a subset of categorical df's columns,\n",
    "            #  in addition of non-categorical columns\n",
    "            cat_df = df.select_dtypes(include=['object'])\n",
    "           \n",
    "            # Identify cat / non-cat columns\n",
    "            list_df = df.columns\n",
    "            list_df_cat = cat_df.columns\n",
    "            list_df_non_cat = []\n",
    "            for var in list_df:\n",
    "                if var not in list_df_cat:\n",
    "                    list_df_non_cat.append(var)\n",
    "            if __debug_script__:\n",
    "                print(\"  - list cat    :\", list_df_cat)\n",
    "                print(\"  - list non cat:\", list_df_non_cat)\n",
    "            del list_df, list_df_cat, list_df_non_cat\n",
    "        except:\n",
    "            print(\"  CAUTION: Unable to find categorical var in df\") \n",
    "            cat_df = pd.DataFrame([])\n",
    "        \n",
    "        if __debug_script__:\n",
    "            print('  - Nb of categorical data detected:', cat_df.shape[1])\n",
    "\n",
    "        # is df empty ? case of no column, whatever rows index\n",
    "        if cat_df.shape[1] == 0: \n",
    "            print(\"  CAUTION: No categorical data found in raw data\") \n",
    "        else:            \n",
    "            for var in cat_df:  # Run along the categorical data columns\n",
    "                if var != response: # We exclude the response var.\n",
    "                    if __debug_script__:\n",
    "                        print(\"  - var '\", var, \"'\")\n",
    "                    try:\n",
    "                        df = pd.concat([df.drop(var, axis=1),\n",
    "                                        pd.get_dummies(df[var],\n",
    "                                        prefix=var, prefix_sep='_',\n",
    "                                        drop_first=False)], axis=1)\n",
    "                        # Initially, I've limited the number of new dummy variables\n",
    "                        #  to create into df to 10 but it was not enough to get\n",
    "                        #  a memory error at this step. Finally, I reduce the\n",
    "                        #  file's content to only 3 years instead of a century.\n",
    "                    except:\n",
    "                        print(\"    CAUTION: Unable to concat cat. var. '\", var, \"'\")\n",
    "                        continue\n",
    "\n",
    "        show_df_shape(df, 'After dummy')\n",
    "        \n",
    "    del cat_df, df_samp, df_col, nb_add_var, df_samp_col,\\\n",
    "        drop_var, add_var, init_df_size, delta\n",
    "    return df\n",
    "\n",
    "# Replace cat column by related dummy columns\n",
    "df = manage_dummy_df(df, categories_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Clean df\n"
     ]
    }
   ],
   "source": [
    "# %%pycodestyle  # Disabled because it raises the error message\n",
    "# \"294:1: W391 blank line at end of file\" while no\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Now that we process with dummy data\n",
    "# we can remove useless rows and columns\n",
    "\n",
    "# CLEAN DATA\n",
    "\n",
    "\n",
    "def remove_empty_row_col(df: object) -> object:  # = df\n",
    "    '''\n",
    "    returns the initial dataframe without empty rows and columns\n",
    "\n",
    "    input:\n",
    "           df : pandas dataframe to filter\n",
    "    ouput:\n",
    "           df : initial dataframe after removal of empty rows and columns\n",
    "    '''\n",
    "\n",
    "    # sub-variables\n",
    "    msg, df_rows_cleaned = None, None\n",
    "\n",
    "    if df is not None:\n",
    "\n",
    "        if __debug_script__:\n",
    "            print(\"  > Remove empty rows and columns\")\n",
    "        # try:\n",
    "        if df is not None:\n",
    "            # Manage rows with none data\n",
    "            # Drop rows with all missing values\n",
    "            df_rows_cleaned = df.dropna(axis=0, how='all')\n",
    "            # Manage columns with none data\n",
    "            # Drop columns with all missing values\n",
    "            df = df_rows_cleaned.dropna(axis=1, how='all')\n",
    "            if __debug_script__:\n",
    "                # Result of the clean ops\n",
    "                msg = \"- df's size after removing empty rows/columns:\"\n",
    "                print(\"    \", msg, df.shape)\n",
    "        # except:\n",
    "        #     print(\"  CAUTION: Unable to remove empty row/column\")\n",
    "\n",
    "    del msg, df_rows_cleaned\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_rows_from_sel_col(df: object, c_name: str) -> object:  # = df\n",
    "    '''\n",
    "    returns the initial dataframe without rows where the selected\n",
    "     column has no value.\n",
    "\n",
    "    input:\n",
    "           df : pandas dataframe to filter\n",
    "    ouput:\n",
    "           df : initial dataframe after removal of empty rows and columns\n",
    "    '''\n",
    "\n",
    "    # sub-variables\n",
    "    msg1, msg2 = None, None\n",
    "\n",
    "    if df is not None:\n",
    "\n",
    "        if __debug_script__:\n",
    "            msg1 = '> Remove rows with missing value from the response column'\n",
    "            print(\"  \", msg1)\n",
    "\n",
    "        # try:\n",
    "        if df is not None:\n",
    "            # remove rows (axis=0 by default) with missing value\n",
    "            #  in response column\n",
    "            df = df.dropna(subset=[c_name])\n",
    "            if __debug_script__:\n",
    "                # Result of the clean ops\n",
    "                msg1 = \"- df's size after removing\"\n",
    "                msg2 = \"empty rows in the sel. column:\"\n",
    "                print(\"    \", msg1, msg2, df.shape)\n",
    "        # except:\n",
    "        #     msg1 = \"CAUTION: Unable to remove \"\n",
    "        #     msg2 = \"missing values from the sel. column.\"\n",
    "        #     print(\"  \", msg1, msg2)\n",
    "\n",
    "    del c_name, msg1, msg2\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_infinite_data(df: object) -> object:  # = df\n",
    "    '''\n",
    "    returns the initial dataframe with no more infinite data: they have\n",
    "     been either removed or replaced according to the case.\n",
    "\n",
    "    input:\n",
    "           df : pandas dataframe to clean\n",
    "    output:\n",
    "           df : df with infinite/invalid values replaced by nan\n",
    "    '''\n",
    "\n",
    "    # sub-variables\n",
    "    var_values, isfinite_sts, values_clean, msg = None, None, None, None\n",
    "\n",
    "    #  column with finite status for related column value\n",
    "\n",
    "    if df is not None:\n",
    "\n",
    "        if __debug_script__:\n",
    "            print(\"    > Manage infinite data\")\n",
    "\n",
    "        # I run over all columns of the dataframe\n",
    "        for var in df.columns:\n",
    "            # content of a column for one given variable\n",
    "            var_values = df[var]\n",
    "\n",
    "            # Identify the type of value than can be infinite\n",
    "            #  it excepts object type which is not concerned\n",
    "            if var_values.dtypes != object:\n",
    "                # Get the finite True/False status for every value\n",
    "                #  of the column\n",
    "                isfinite_sts = np.isfinite(var_values)\n",
    "                # I apply a treatment when not all values are finite\n",
    "                if not np.all(isfinite_sts):\n",
    "                    if __debug_script__:\n",
    "                        print(\"      - \", var, \"contains infinite values\")\n",
    "                    # try:\n",
    "                    # I have to distinguish type 'float64' that I met\n",
    "                    #  on a previous dataset I worked with because\n",
    "                    #  it has required a specific treatment,\n",
    "                    #  full removal because no other solution worked\n",
    "                    if var_values.dtypes == 'float64':\n",
    "                        # Drop/Remove var from df\n",
    "                        df = df.drop(columns=[var])\n",
    "                        if __debug_script__:\n",
    "                            print(\"        - float64 variable removed\")\n",
    "\n",
    "                    # otherwise, I can replace infinite value by Nan\n",
    "                    else:\n",
    "                        values_clean = np.where(not isfinite_sts,\n",
    "                                                np.nan, var_values)\n",
    "                        df[var] = pd.Series(values_clean)\n",
    "                        if __debug_script__:\n",
    "                            msg = '- Infinite val. replaced by Nan'\n",
    "                            print(\"       \", msg)\n",
    "                    # except:\n",
    "                    #     msg = 'CAUTION: Unable to replace infinite values on'\n",
    "                    #     print(\"      \", msg, var, ' (', a.dtypes, ')')\n",
    "                    #     continue\n",
    "\n",
    "        if __debug_script__:\n",
    "            msg = \"- df's size after removal of infinite values:\"\n",
    "            print(\"    \", msg, df.shape)\n",
    "\n",
    "    del var_values, isfinite_sts, values_clean, msg\n",
    "    return df\n",
    "\n",
    "\n",
    "def manage_missing_num(df: object) -> object:  # = df\n",
    "    '''\n",
    "    returns the initial dataframe with null values of numerical column\n",
    "     replaced by the appopropriate of the column\n",
    "\n",
    "    input:\n",
    "           df : initial pandas dataframe\n",
    "    output:\n",
    "           df : initial dataframe with missing values of\n",
    "                numerical column which are replaced by\n",
    "                the appropriate value\n",
    "    '''\n",
    "\n",
    "    # sub-variables\n",
    "    msg, num_col_list, c_name = None, None, None\n",
    "    null_val_rate, l_ref = None, None\n",
    "\n",
    "    if df is not None:\n",
    "\n",
    "        if __debug_script__:\n",
    "            print(\"  > Manage missing numerical data\")\n",
    "\n",
    "        # I identify columns with numerical data (float and int types)\n",
    "        num_col_list = df.select_dtypes(include=['float', 'int']).columns\n",
    "        # When the list of numerical columns is not empty\n",
    "        #  i.e. there is numerical column to check\n",
    "        if len(num_col_list) > 0:\n",
    "            if __debug_script__:\n",
    "                print(\"      List of num. columns:\", num_col_list)\n",
    "            # I run the list, column by column\n",
    "            for c_name in num_col_list:\n",
    "                if __debug_script__:\n",
    "                    print(\"      > column '\", c_name, \"'\")\n",
    "                # When at least one value of the column is null (Nan ...),\n",
    "                #  When the number of null value is higher than 50%\n",
    "                #  of the total number of value of the column,\n",
    "                #  I assumed not to have enough information to replace\n",
    "                #  null value by the median, so I replace null value by\n",
    "                #  the most frequent value,\n",
    "                #  otherwise, I replace it by the median value of\n",
    "                #  the column\n",
    "                #  I use median or most frequent value to ensure having\n",
    "                #  integer values as other values of the column since,\n",
    "                #  here, we are talking about number of people (no float)\n",
    "                if df[c_name].isnull().values.any():\n",
    "                    # try:\n",
    "                    l_ref = len(df[c_name])\n",
    "                    null_val_rate = df[c_name].isnull().sum() / l_ref\n",
    "                    if __debug_script__:\n",
    "                        msg = '- Null value ratio of'\n",
    "                        print(\"        \", null_val_rate)\n",
    "                    if null_val_rate > 0.5:\n",
    "                        subst_val = df.mode(axis=0, numeric_only=True)\n",
    "                        if __debug_script__:\n",
    "                            msg = '- Add most frequent value on:'\n",
    "                            print(\"        \", c_name)\n",
    "                    else:\n",
    "                        subst_val = df[c_name].median()\n",
    "                        if __debug_script__:\n",
    "                            msg = '- Add median value on:'\n",
    "                            print(\"        \", c_name)\n",
    "                    df[c_name].fillna(subst_val, inplace=True)\n",
    "                    # except:\n",
    "                    #     if __debug_script__:\n",
    "                    #         msg = 'CAUTION: Unable to fill the mean for var'\n",
    "                    #         print(\"      \", msg, \"'\", col, \"'\")\n",
    "                else:\n",
    "                    if __debug_script__:\n",
    "                        print(\"        - No null value detected\")\n",
    "        else:\n",
    "            if __debug_script__:\n",
    "                print(\"      - No float/int variable found in df\")\n",
    "\n",
    "    del msg, num_col_list, c_name\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_df(df: object, categories_selection: dict) -> object:  # = df\n",
    "    '''\n",
    "    returns the initial dataframe cleaned as desired, after management\n",
    "     of missing, null and infinite data\n",
    "\n",
    "    input:\n",
    "           df : pandas dataframe\n",
    "    ouput:\n",
    "           df : input dataframe after undesirable data cleaning\n",
    "    '''\n",
    "    # function : remove infinite values\n",
    "\n",
    "    # sub-variables\n",
    "    msg = None\n",
    "\n",
    "    print(\"> Clean df\")  # Manage missing data\n",
    "    if __debug_script__:\n",
    "        print(\"  - df's size before cleaning:\", df.shape)\n",
    "\n",
    "    # Remove complete missing data\n",
    "    # I decided to remove rows and columns that do not contain\n",
    "    #  any value; they would bring nothing to the building of the\n",
    "    #  model, except take time and memory space for nothing.\n",
    "    df = remove_empty_row_col(df)\n",
    "\n",
    "    # Check that response is still in the dataframe\n",
    "    #  after removal of all empty rows and columns\n",
    "    # It may be useful to now that when you change the response\n",
    "    response = categories_selection['response']\n",
    "    if response not in df.columns:\n",
    "        msg = 'CAUTION: the Response variable'\n",
    "        print(\"  \", msg, \"'\", response, \"' is not available in the df\")\n",
    "        df = None\n",
    "\n",
    "    # Drop rows with missing response values\n",
    "    # I decided to remove row of all columns when related value in the\n",
    "    #  response column is missing since then, in the frame of the model\n",
    "    #  building, I would have information to assess the response, but\n",
    "    #  no response to linked with.\n",
    "    df = remove_rows_from_sel_col(df, response)\n",
    "\n",
    "    # Clean infinite values\n",
    "    #  because infinite values may not be properly managed by the code.\n",
    "    #  They have no reason to be here and they cause memory errors.\n",
    "    df = clean_infinite_data(df)\n",
    "\n",
    "    # Manage null values of the numerical column\n",
    "    #  because it may happen that hospitals do not\n",
    "    #  fulfill all information once but, here we work on\n",
    "    #  a lot of different hospitals and the same at different time\n",
    "    #  so I assume that I can complete some blanks.\n",
    "    #  Here fortunately, I do not have null value.\n",
    "    df = manage_missing_num(df)\n",
    "\n",
    "    del categories_selection, response, msg\n",
    "    return df\n",
    "\n",
    "\n",
    "# Clean data\n",
    "# drop rows/columns with all missing values\n",
    "# + replace infinite values by Nan\n",
    "df = clean_df(df, categories_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Split data into X/y\n"
     ]
    }
   ],
   "source": [
    "# %%pycodestyle  # Disabled because raised an error message\n",
    "# \"75:1: W391 blank line at end of file\", while no\n",
    "\n",
    "# PREPARING FOR MODELING\n",
    "\n",
    "# Now that we clean the dataframe\n",
    "# we can prepare the dataset for modeling\n",
    "\n",
    "\n",
    "def get_X_y(df: object, categories_selection: dict):\n",
    "    '''\n",
    "    returns dataframes of response y and input data X for aiming at\n",
    "     defining a model\n",
    "\n",
    "    input:\n",
    "           df       : pandas dataframe\n",
    "           response : response column's name\n",
    "    ouput:\n",
    "           X : A matrix holding all of the variables you want to consider\n",
    "                when predicting the response\n",
    "           y : the corresponding response vector\n",
    "    '''\n",
    "    # function : split df into exploratory X data and response y data\n",
    "\n",
    "    # output\n",
    "    X, y = None, None\n",
    "\n",
    "    # sub-variables\n",
    "    response = categories_selection['response']\n",
    "\n",
    "    print(\"> Split data into X/y\")\n",
    "    if __debug_script__:\n",
    "        print(\"  - response:\", response)\n",
    "        print(\"  - df.columns:\", df.columns)\n",
    "\n",
    "    # Get the Response var\n",
    "    if __debug_script__:\n",
    "        print(\"  > Get y\")\n",
    "    if response in df.columns:\n",
    "        # try:\n",
    "        # Split into explanatory and response variables (1/2)\n",
    "        #  Get response variable\n",
    "        y = df[response]\n",
    "        df = df.drop(columns=[response])  # Remove pred_name from df\n",
    "        # except:\n",
    "        #     print(\"    CAUTION: Unable to get the response data in df\")\n",
    "        #     y = None\n",
    "    else:\n",
    "        print(\"    CAUTION: Unable to find the response in df\")\n",
    "        y = None\n",
    "\n",
    "    if __debug_script__ and (y is not None):\n",
    "        print(\"    - y shape:\", y.shape)\n",
    "\n",
    "    # Get the Exploratory vars\n",
    "    if __debug_script__:\n",
    "        print(\"  > Get X\")\n",
    "    # try:\n",
    "    # Split into explanatory and response variables (2/2)\n",
    "    #  Get the input variables i.e. at this level just a copy of df\n",
    "    X = df.copy(deep=True)\n",
    "    # except:\n",
    "    #     print(\"    CAUTION: Unable to get the exploratory variables (X)\")\n",
    "    #     X = None\n",
    "\n",
    "    if __debug_script__ and (X is not None):\n",
    "        print(\"    - X shape:\", X.shape)\n",
    "\n",
    "    del df, categories_selection, response\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Split into Response y / Exploratory variables X\n",
    "X, y = get_X_y(df, categories_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Get model\n",
      "  > Split Train / Test\n",
      "  > Type of data\n",
      "  > Modeling\n",
      "  > Metrics\n",
      "    - r2_scores_train: 0.9761588302089482\n",
      "    - r2_scores_test: 0.9755838001373758\n"
     ]
    }
   ],
   "source": [
    "# %%pycodestyle  # Disabled because raised an error message\n",
    "# 162:1: W391 blank line at end of file, while no\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Now that we prepare dataset for modeling,\n",
    "# it's time to model\n",
    "\n",
    "# MODELING\n",
    "\n",
    "\n",
    "def get_model(X: object, y: object, testrate=.3):\n",
    "    '''\n",
    "    returns a linear model, set of data used to establish,\n",
    "     test this model and the associated r2 scores.\n",
    "\n",
    "    input:\n",
    "           X          : explanatory variables object\n",
    "           y          : response variable object\n",
    "           testrate   : proportion of the dataset to include in\n",
    "                         the test split,between 0.0 and 1.0;\n",
    "                         default value = 0.3\n",
    "    output:\n",
    "            model : linear regression model object from sklearn\n",
    "            score : Merge of mean square error value between Train &\n",
    "                     Test data set according to the proposed model\n",
    "            list of X_train and y_train\n",
    "            list of X_test and y_test\n",
    "    '''\n",
    "    # function: Give a prediction model according to\n",
    "    #  its X/y_test/train inputs/outputs values\n",
    "\n",
    "    model, score = None, 0\n",
    "    Xtrain, Xtest, ytrain, ytest = None, None, None, None\n",
    "\n",
    "    # sub-variables\n",
    "    y_pred, acc_score_train, acc_score_test, msg = None, None, None, None\n",
    "    r2_scores_train, r2_scores_test, mdl_score = None, None, None\n",
    "\n",
    "    if (X is not None) and (y is not None):\n",
    "\n",
    "        print(\"> Get model\")\n",
    "\n",
    "        # Split into train and test X/y data set\n",
    "        #  to establish the model and score it\n",
    "        print(\"  > Split Train / Test\")\n",
    "        # try:\n",
    "        Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,\n",
    "                                                        test_size=testrate,\n",
    "                                                        random_state=42)\n",
    "        if __debug_script__:\n",
    "            print(\"    - X/y size:\", X.shape, y.shape)\n",
    "            print(\"    - Train X/y size:\", Xtrain.shape, ytrain.shape)\n",
    "            print(\"    - Test  X/y size:\", Xtest.shape, ytest.shape)\n",
    "        split = True\n",
    "        # except:\n",
    "        #     msg = 'CAUTION: Unable to split X/y into train & test dataset'\n",
    "        #     print(\"  \", msg)\n",
    "        #     split = False\n",
    "\n",
    "        # Work on dtype\n",
    "        print(\"  > Type of data\")\n",
    "\n",
    "        if __debug_script__:\n",
    "            print(\"    - y:\", y.dtypes)\n",
    "\n",
    "        recensed_type = {}\n",
    "        for var in X.columns:\n",
    "            tip = X[var].dtypes\n",
    "            if tip not in recensed_type:\n",
    "                recensed_type[tip] = 1\n",
    "            else:\n",
    "                recensed_type[tip] = recensed_type[tip] + 1\n",
    "        if len(recensed_type) > 0:\n",
    "            for key, value in recensed_type.items():\n",
    "                if __debug_script__:\n",
    "                    print(\"    - X\", key, \" : x\", value)\n",
    "            if __debug_script__:\n",
    "                print(\"    - Over\", X.shape[1], \" columns\")\n",
    "\n",
    "        # Establish model\n",
    "        print(\"  > Modeling\")\n",
    "        if split:\n",
    "            if __debug_script__:\n",
    "                print(\"    - Train X/y size:\", Xtrain.shape, ytrain.shape)\n",
    "\n",
    "            # https://scikit-learn.org/stable/modules/generated/sklearn.linear\n",
    "            #  _model.LinearRegression.html#sklearn.linear_model.\n",
    "            #  LinearRegression.predict\n",
    "            # https://www.codegrepper.com/code-examples/python/reg.predict+python\n",
    "            # Fit linear model\n",
    "            #  further methods:\n",
    "            #  https://scikit-learn.org/stable/modules/linear_model.html\n",
    "            model = LinearRegression().fit(Xtrain, ytrain)\n",
    "\n",
    "            # https://scikit-learn.org/stable/modules/linear_model.\n",
    "            #  html#ridge-regression-and-classification\n",
    "            # and https://scikit-learn.org/stable/modules/generated/sklearn.\n",
    "            #  linear_model.Ridge.html\n",
    "            # clf = linear_model.Ridge(alpha=1.0)\n",
    "            # Ridge(alpha=1.0)\n",
    "            # model = clf.fit(X_train, y_train)\n",
    "            # It does not work!\n",
    "\n",
    "            # Get the model's score\n",
    "            # Return the coefficient of determination of the prediction\n",
    "            mdl_score = model.score(Xtrain, ytrain)\n",
    "\n",
    "            if __debug_script__:\n",
    "                print(\"    - Train mdl_score:\", mdl_score)\n",
    "            if __debug_script__:\n",
    "                print(\"    - Test  X/y size:\", Xtest.shape, ytest.shape)\n",
    "            y_pred = model.predict(Xtest)\n",
    "        else:\n",
    "            y_pred = None\n",
    "        del split\n",
    "\n",
    "        # Evaluate this model\n",
    "        if y_pred is None:\n",
    "            if __debug_script__:\n",
    "                print(\"    - Model not found\")\n",
    "\n",
    "        # Get metrics with a model by Regression\n",
    "        print(\"  > Metrics\")\n",
    "        if y_pred is not None:\n",
    "            # Accuracy_score (https://scikit-learn.org/stable/modules/\n",
    "            # generated/sklearn.metrics.accuracy_score.html)\n",
    "            # Confusion matrix (https://scikit-learn.org/stable/modules/\n",
    "            # generated/sklearn.metrics.confusion_matrix.html)\n",
    "            #  not appropriate for my purpose\n",
    "            # Common pitfalls (https://scikit-learn.org/stable/common_\n",
    "            #  pitfalls.html): mean_sqaured_error and r2_score\n",
    "\n",
    "            # According to https://stackoverflow.com/questions/37367405/\n",
    "            #  python-scikit-learn-cant-handle-mix-of-multiclass-and-continuous\n",
    "            # ... Accuracy score is only for classification problems\n",
    "            # acc_score_train = accuracy_score(y_train, model.predict(X_train))\n",
    "            # acc_score_test = accuracy_score(y_test, model.predict(X_test))\n",
    "            # if debug: print(\"    - acc_score_train:\", acc_score_train)\n",
    "            # if debug: print(\"    - acc_score_test:\", acc_score_test)\n",
    "\n",
    "            # always according to https://stackoverflow.com/questions/37367405/\n",
    "            #  python-scikit-learn-cant-handle-mix-of-multiclass-and-continuous\n",
    "            # For regression problems, use: R2 Score, MSE (Mean Squared Error),\n",
    "            # RMSE (Root Mean Squared Error).\n",
    "            r2_scores_train = r2_score(ytrain, model.predict(Xtrain))\n",
    "            r2_scores_test = r2_score(ytest, model.predict(Xtest))\n",
    "            print(\"    - r2_scores_train:\", r2_scores_train)\n",
    "            print(\"    - r2_scores_test:\", r2_scores_test)\n",
    "\n",
    "            score = r2_scores_train\n",
    "\n",
    "    del X, y, testrate, y_pred, acc_score_train, acc_score_test\n",
    "    del r2_scores_train, r2_scores_test, mdl_score\n",
    "    return model, score, [Xtrain, ytrain], [Xtest, ytest]\n",
    "\n",
    "\n",
    "# modeling\n",
    "model, score, Xy_train, Xy_test = get_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Define impact of categories on the model:\n",
      "                                     est_int         coefs     abs_coefs\n",
      "107                           Sexe_Tous  4.629496e+08  4.629496e+08\n",
      "106                          Sexe_Homme  4.629496e+08  4.629496e+08\n",
      "105                          Sexe_Femme  4.629495e+08  4.629495e+08\n",
      "60               Code_du_Departement_57  2.284012e+08  2.284012e+08\n",
      "78               Code_du_Departement_75  2.284010e+08  2.284010e+08\n",
      "97               Code_du_Departement_94  2.284009e+08  2.284009e+08\n",
      "62               Code_du_Departement_59  2.284009e+08  2.284009e+08\n",
      "63               Code_du_Departement_60  2.284009e+08  2.284009e+08\n",
      "71               Code_du_Departement_68  2.284008e+08  2.284008e+08\n",
      "98               Code_du_Departement_95  2.284008e+08  2.284008e+08\n",
      "70               Code_du_Departement_67  2.284008e+08  2.284008e+08\n",
      "65               Code_du_Departement_62  2.284008e+08  2.284008e+08\n",
      "83               Code_du_Departement_80  2.284008e+08  2.284008e+08\n",
      "8                Code_du_Departement_06  2.284008e+08  2.284008e+08\n",
      "86               Code_du_Departement_83  2.284008e+08  2.284008e+08\n",
      "81               Code_du_Departement_78  2.284008e+08  2.284008e+08\n",
      "4                Code_du_Departement_02  2.284008e+08  2.284008e+08\n",
      "91               Code_du_Departement_88  2.284008e+08  2.284008e+08\n",
      "80               Code_du_Departement_77  2.284008e+08  2.284008e+08\n",
      "57               Code_du_Departement_54  2.284008e+08  2.284008e+08\n",
      "41               Code_du_Departement_38  2.284008e+08  2.284008e+08\n",
      "45               Code_du_Departement_42  2.284008e+08  2.284008e+08\n",
      "72               Code_du_Departement_69  2.284007e+08  2.284007e+08\n",
      "74               Code_du_Departement_71  2.284007e+08  2.284007e+08\n",
      "79               Code_du_Departement_76  2.284007e+08  2.284007e+08\n",
      "54               Code_du_Departement_51  2.284007e+08  2.284007e+08\n",
      "27               Code_du_Departement_26  2.284007e+08  2.284007e+08\n",
      "93               Code_du_Departement_90  2.284007e+08  2.284007e+08\n",
      "20               Code_du_Departement_18  2.284007e+08  2.284007e+08\n",
      "66               Code_du_Departement_63  2.284007e+08  2.284007e+08\n",
      "22               Code_du_Departement_21  2.284007e+08  2.284007e+08\n",
      "28               Code_du_Departement_27  2.284007e+08  2.284007e+08\n",
      "92               Code_du_Departement_89  2.284007e+08  2.284007e+08\n",
      "5                Code_du_Departement_03  2.284007e+08  2.284007e+08\n",
      "10               Code_du_Departement_08  2.284007e+08  2.284007e+08\n",
      "99              Code_du_Departement_971  2.284007e+08  2.284007e+08\n",
      "96               Code_du_Departement_93  2.284007e+08  2.284007e+08\n",
      "87               Code_du_Departement_84  2.284007e+08  2.284007e+08\n",
      "55               Code_du_Departement_52  2.284007e+08  2.284007e+08\n",
      "76               Code_du_Departement_73  2.284007e+08  2.284007e+08\n",
      "13               Code_du_Departement_11  2.284007e+08  2.284007e+08\n",
      "58               Code_du_Departement_55  2.284007e+08  2.284007e+08\n",
      "3                Code_du_Departement_01  2.284007e+08  2.284007e+08\n",
      "61               Code_du_Departement_58  2.284007e+08  2.284007e+08\n",
      "42               Code_du_Departement_39  2.284007e+08  2.284007e+08\n",
      "94               Code_du_Departement_91  2.284007e+08  2.284007e+08\n",
      "9                Code_du_Departement_07  2.284007e+08  2.284007e+08\n",
      "100             Code_du_Departement_972  2.284007e+08  2.284007e+08\n",
      "53               Code_du_Departement_50  2.284007e+08  2.284007e+08\n",
      "90               Code_du_Departement_87  2.284007e+08  2.284007e+08\n",
      "84               Code_du_Departement_81  2.284007e+08  2.284007e+08\n",
      "33               Code_du_Departement_30  2.284007e+08  2.284007e+08\n",
      "19               Code_du_Departement_17  2.284006e+08  2.284006e+08\n",
      "39               Code_du_Departement_36  2.284006e+08  2.284006e+08\n",
      "12               Code_du_Departement_10  2.284006e+08  2.284006e+08\n",
      "77               Code_du_Departement_74  2.284006e+08  2.284006e+08\n",
      "69               Code_du_Departement_66  2.284006e+08  2.284006e+08\n",
      "67               Code_du_Departement_64  2.284006e+08  2.284006e+08\n",
      "40               Code_du_Departement_37  2.284006e+08  2.284006e+08\n",
      "95               Code_du_Departement_92  2.284006e+08  2.284006e+08\n",
      "16               Code_du_Departement_14  2.284006e+08  2.284006e+08\n",
      "25               Code_du_Departement_24  2.284006e+08  2.284006e+08\n",
      "75               Code_du_Departement_72  2.284006e+08  2.284006e+08\n",
      "43               Code_du_Departement_40  2.284006e+08  2.284006e+08\n",
      "14               Code_du_Departement_12  2.284006e+08  2.284006e+08\n",
      "44               Code_du_Departement_41  2.284006e+08  2.284006e+08\n",
      "47               Code_du_Departement_44  2.284006e+08  2.284006e+08\n",
      "6                Code_du_Departement_04  2.284006e+08  2.284006e+08\n",
      "48               Code_du_Departement_45  2.284006e+08  2.284006e+08\n",
      "64               Code_du_Departement_61  2.284006e+08  2.284006e+08\n",
      "51               Code_du_Departement_48  2.284006e+08  2.284006e+08\n",
      "52               Code_du_Departement_49  2.284006e+08  2.284006e+08\n",
      "73               Code_du_Departement_70  2.284006e+08  2.284006e+08\n",
      "56               Code_du_Departement_53  2.284006e+08  2.284006e+08\n",
      "82               Code_du_Departement_79  2.284006e+08  2.284006e+08\n",
      "21               Code_du_Departement_19  2.284006e+08  2.284006e+08\n",
      "26               Code_du_Departement_25  2.284006e+08  2.284006e+08\n",
      "18               Code_du_Departement_16  2.284006e+08  2.284006e+08\n",
      "85               Code_du_Departement_82  2.284006e+08  2.284006e+08\n",
      "50               Code_du_Departement_47  2.284006e+08  2.284006e+08\n",
      "31               Code_du_Departement_2A  2.284006e+08  2.284006e+08\n",
      "59               Code_du_Departement_56  2.284006e+08  2.284006e+08\n",
      "46               Code_du_Departement_43  2.284006e+08  2.284006e+08\n",
      "17               Code_du_Departement_15  2.284006e+08  2.284006e+08\n",
      "35               Code_du_Departement_32  2.284006e+08  2.284006e+08\n",
      "7                Code_du_Departement_05  2.284006e+08  2.284006e+08\n",
      "49               Code_du_Departement_46  2.284006e+08  2.284006e+08\n",
      "37               Code_du_Departement_34  2.284006e+08  2.284006e+08\n",
      "89               Code_du_Departement_86  2.284006e+08  2.284006e+08\n",
      "11               Code_du_Departement_09  2.284006e+08  2.284006e+08\n",
      "68               Code_du_Departement_65  2.284006e+08  2.284006e+08\n",
      "30               Code_du_Departement_29  2.284006e+08  2.284006e+08\n",
      "32               Code_du_Departement_2B  2.284006e+08  2.284006e+08\n",
      "38               Code_du_Departement_35  2.284006e+08  2.284006e+08\n",
      "24               Code_du_Departement_23  2.284006e+08  2.284006e+08\n",
      "29               Code_du_Departement_28  2.284006e+08  2.284006e+08\n",
      "88               Code_du_Departement_85  2.284006e+08  2.284006e+08\n",
      "23               Code_du_Departement_22  2.284006e+08  2.284006e+08\n",
      "104             Code_du_Departement_978  2.284006e+08  2.284006e+08\n",
      "36               Code_du_Departement_33  2.284006e+08  2.284006e+08\n",
      "103             Code_du_Departement_976  2.284005e+08  2.284005e+08\n",
      "102             Code_du_Departement_974  2.284005e+08  2.284005e+08\n",
      "34               Code_du_Departement_31  2.284004e+08  2.284004e+08\n",
      "101             Code_du_Departement_973  2.284004e+08  2.284004e+08\n",
      "15               Code_du_Departement_13  2.284003e+08  2.284003e+08\n",
      "1    Nb_actuellement_en_soins_intensifs  2.209095e+00  2.209095e+00\n",
      "0          Nb_actuellement_hospitalises -2.550077e-01  2.550077e-01\n",
      "2               Total_retour_a_domicile  1.727804e-01  1.727804e-01\n"
     ]
    }
   ],
   "source": [
    "# %%pycodestyle # Disabled because it raises an error message\n",
    "# 57:1: W391 blank line at end of file, while no\n",
    "\n",
    "# ANALYZE THE MODEL\n",
    "\n",
    "# Now that we have a model, it's time ask some questions\n",
    "\n",
    "# First question\n",
    "# Which categories have the most effect on the model ?\n",
    "\n",
    "\n",
    "def coef_weights(model, X_train) -> object:\n",
    "    '''\n",
    "    returns a dataframe with coefficients of the model\n",
    "     (real and absolute values) sorted in the descending order\n",
    "     of the absolute values\n",
    "\n",
    "    input:\n",
    "           model     : model for which we are looking coefficients\n",
    "           X_train   : the training data\n",
    "    output:\n",
    "            coefs_df : dataframe with model's coefficients; that can be\n",
    "                        used to understand the most influential coefficients\n",
    "                        in a linear model by providing the coefficient\n",
    "                        estimates along with the name of the variable\n",
    "                        attached to the coefficient.\n",
    "    '''\n",
    "    # function: get model's coefficients\n",
    "\n",
    "    coefs_df = pd.DataFrame()\n",
    "    # Get name of every column in front  of its coefficients\n",
    "    coefs_df['est_int'] = X_train.columns\n",
    "    # get coefficients of the linear model\n",
    "    coefs_df['coefs'] = model.coef_\n",
    "    # get absolute value of these coefficients\n",
    "    coefs_df['abs_coefs'] = np.abs(model.coef_)\n",
    "    # Sort coefficient by descending order\n",
    "    coefs_df = coefs_df.sort_values('abs_coefs', ascending=False)\n",
    "\n",
    "    del model, X_train\n",
    "    return coefs_df\n",
    "\n",
    "\n",
    "print('  > Define impact of categories on the model:')\n",
    "coef_df = None\n",
    "\n",
    "# Compute coefficient of weight on every category for this model\n",
    "coef_df = coef_weights(model, Xy_train[0])  # Xy_train = [Xtrain, ytrain]\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "# If max_rows is exceeded, switch to truncate view\n",
    "# If max_cols is exceeded, switch to truncate view\n",
    "print('    ', coef_df)\n",
    "\n",
    "# Result\n",
    "# display of model's weight coefficients show that gender of patients\n",
    "#  is the first category of the model (mortality), before department code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  > Provide stats related to rate of patients in critical care over hospitalization :\n",
      "    - Stats computed on 13.81 % ( 38739 values) of the full dataset\n",
      "    - maximum: 80.00 % \n",
      "    - average: 12.23 % \n",
      "    - median : 10.13 %\n",
      "    - It may concern 49.99 % of COVID-19 patients when considering 99.65 % of the less critical patients (for an initial target at 99.70 %)\n",
      "    - It may concern 30.76 % of COVID-19 patients when considering 95.36 % of the less critical patients (for an initial target at 95.40 %)\n",
      "    - It may concern 15.15 % of COVID-19 patients when considering 68.26 % of the less critical patients (for an initial target at 68.30 %)\n",
      "  > Provide stats related to rate of female that died due to covid-19 :\n",
      "    - Stats computed on 5.30 % ( 14870 values) of the full dataset\n",
      "    - maximum: 10.00 % \n",
      "    - average: 0.41 % \n",
      "    - median : 0.00 %\n",
      "    - It may concern 9.09 % of COVID-19 patients when considering 99.66 % of the less critical patients (for an initial target at 99.70 %)\n",
      "    - It may concern 2.43 % of COVID-19 patients when considering 95.46 % of the less critical patients (for an initial target at 95.40 %)\n",
      "    - It may concern 0.08 % of COVID-19 patients when considering 68.45 % of the less critical patients (for an initial target at 68.30 %)\n",
      "  > Provide stats related to rate of male that died due to covid-19 :\n",
      "    - Stats computed on 5.30 % ( 14870 values) of the full dataset\n",
      "    - maximum: 10.00 % \n",
      "    - average: 0.34 % \n",
      "    - median : 0.00 %\n",
      "    - It may concern 9.09 % of COVID-19 patients when considering 99.61 % of the less critical patients (for an initial target at 99.70 %)\n",
      "    - It may concern 1.92 % of COVID-19 patients when considering 95.38 % of the less critical patients (for an initial target at 95.40 %)\n",
      "    - It may concern 0.07 % of COVID-19 patients when considering 68.55 % of the less critical patients (for an initial target at 68.30 %)\n",
      "\n",
      "---------- PROJECT #01 /  END  ---------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%pycodestyle # Disabled because it raises an error message\n",
    "#  \"too many values to unpack (expected 3)\" linked to pycodestyle\n",
    "# See above at the beginning.\n",
    "\n",
    "# STATS ANALYSIS\n",
    "\n",
    "# Finally, I'd like to have the stats over recorded patients\n",
    "# Since the principe will be the same, only categories will change,\n",
    "#  I gather three questions here that will be treated in sequence\n",
    "#  the same way.\n",
    "\n",
    "# Questions:\n",
    "# What is the proportion of hospitalized Covid-19 patients\n",
    "#  which go in critical care ?\n",
    "# According to the result of the preceding block that raises\n",
    "#  the importance of the gender for the model,\n",
    "#  what is the proportion of female and male Covid-19 patients\n",
    "#   that finally died due to Covid-19 ?\n",
    "\n",
    "\n",
    "def df_logical_no(df, c_name):\n",
    "    '''\n",
    "    returns the initial dataframe completed with a new column\n",
    "     with result of a logical no operation on a selected data,\n",
    "     reporting also the new column's name\n",
    "\n",
    "    input:\n",
    "      df         : initial dataframe to modify\n",
    "      c_name: name of the df's column to compute a 'logical no' for\n",
    "    output:\n",
    "      df : df completed with the 'logical no' column of the given column\n",
    "      new_c_name : name of the new column\n",
    "    '''\n",
    "    # function: add values of a 'logical no' column for a given category\n",
    "    #  of the dataframe\n",
    "\n",
    "    liste, r, no_b, msg = None, None, None, None\n",
    "    if __debug_script__:\n",
    "        print('   - LOGICAL NO:', c_name)\n",
    "    # try:\n",
    "    # Create a list that gives result of a boolean logical no,\n",
    "    #  returning None for None value\n",
    "    liste = df[c_name].values.tolist()\n",
    "    for i, j in enumerate(liste):\n",
    "        if j is None:\n",
    "            liste[i] = -1\n",
    "    complement = np.add(liste, 1)\n",
    "    no_liste = np.where(complement > 1, False,\n",
    "                        np.where(complement == 0, None, True))\n",
    "\n",
    "    # Implement the new column with its name into the dataframe\n",
    "    new_c_name = c_name + '_logical_no'\n",
    "    df[new_c_name] = pd.Series(no_liste)\n",
    "    if __debug_script__:\n",
    "        print('     - df: -> no', column, '\\n')\n",
    "    # except:\n",
    "    #     msg = '- Unable to compute logical no of column'\n",
    "    #     print(\"    \", msg, \"'\" + column + \"'\")\n",
    "\n",
    "    del column_name, liste, r, no_b, msg\n",
    "    return df, new_c_name\n",
    "\n",
    "\n",
    "def divide_N_by_D(df, col_ND_names, new_c_name, min_qty_col_D) -> object:\n",
    "    '''\n",
    "    returns the initial dataframe completed with a new column\n",
    "     containing the required divide values\n",
    "\n",
    "    input:\n",
    "      df          : initial dataframe\n",
    "      col_ND_names: names of df's columns for numerator of the division\n",
    "                     and denominator of the division\n",
    "      new_c_name: name of new df's column with result of the division\n",
    "     min_qty_col_D: Minimum value to consider on denominator do perform\n",
    "                     the computation\n",
    "    output:\n",
    "      df          : df completed with result of the divide\n",
    "    '''\n",
    "    # function: divide values of two selected columns from a df\n",
    "    #            and add result into this df\n",
    "\n",
    "    division = []\n",
    "\n",
    "    # Get numerator and denominator values\n",
    "    list_N = df[col_ND_names[0]].values.tolist()\n",
    "    list_D = df[col_ND_names[1]].values.tolist()\n",
    "\n",
    "    # Run the list, divide index by index with requested management\n",
    "    for i, numerator in enumerate(list_N):\n",
    "        if list_D[i] != 0:\n",
    "            if list_D[i] >= min_qty_col_D:\n",
    "                r = 100 * numerator / list_D[i]  # in percent\n",
    "                # For debug, display start & too high values of computation\n",
    "                if __debug_script__ and ((i < 10) or (r > 99.9)):\n",
    "                    print('i=', i, '->', list_N[i], '/', list_D[i],\n",
    "                          '=', r, '%')\n",
    "            else:\n",
    "                r = None\n",
    "        else:\n",
    "            r = None\n",
    "        division.append(r)\n",
    "    # Tring to process with numpy functions and list/np.Array did not work\n",
    "    # it generates more issues to deal with such it was cancelled.\n",
    "\n",
    "    df[new_c_name] = pd.Series(division)  # Add result in df\n",
    "    # Sort df in descending order by result of the divide\n",
    "    df = df.sort_values(new_c_name, ascending=False)\n",
    "\n",
    "    del col_ND_names, new_c_name, list_N, list_D,\n",
    "    del division, min_qty_col_D\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_value_at_tgt_quantil(df:object, c_name: str, tgt_qtil: float,\n",
    "                             nb_valid_data=None):\n",
    "    '''\n",
    "    returns a quantil value and the associated selected data value\n",
    "\n",
    "    input:\n",
    "      df           : dataframe to work on\n",
    "      c_name     : name of the df's column to work on\n",
    "      tgt_qtil     : target quantil (%)\n",
    "      nb_valid_data: (option) size of the valid dataset to consider\n",
    "                      in the column; default = df_column's full size\n",
    "    output:\n",
    "      found_qtil   : actual quantil value (normally close to the\n",
    "                      tgt_qtil) that reach the closest the target quantil\n",
    "      found_value  : value of the data that reach the tgt_qtil\n",
    "                      of data at or below this value\n",
    "    '''\n",
    "    # function: Find the quantil value closest to the target quantil of \n",
    "    #            the selected df column's dataset\n",
    "\n",
    "\n",
    "    # Primary variables declaration\n",
    "    found_value, found_quantil = 0, 0\n",
    "    # sub-variables declaration\n",
    "    delta, min_delta = 100, 100\n",
    "    moving_value_int, moving_value, moving_qtil = 0, 0, 0\n",
    "\n",
    "    data = df[c_name]  # Get df column's data to work on\n",
    "\n",
    "    if nb_valid_data is None:  # get a default value of valid data if needed\n",
    "        nb_valid_data = len(data)\n",
    "\n",
    "    # We run the 'moving_value' from 0 to 100% by 0.01%-step\n",
    "    for moving_value_int in range(0, 10000, 1):\n",
    "        moving_value = float(moving_value_int)/100\n",
    "        # Get 'moving quantil' of 'value' below the 'moving_value'\n",
    "        moving_qtil = 100 * data[data <= moving_value].count() / nb_valid_data\n",
    "        # Compute the diffrence btw 'moving_qtil' and 'tgt_qtil'\n",
    "        #  we are trying to reach\n",
    "        delta = abs(tgt_qtil - moving_qtil)\n",
    "        if __debug_script__:\n",
    "            print('    for moving_value:', \"{:.3f}\".format(moving_value),\n",
    "                  '\\t we reach moving_quantil:', \"{:.3f}\".format(moving_qtil),\n",
    "                  ' => delta ', \"{:.3f}\".format(delta),\n",
    "                  'vs min_delta:', \"{:.3f}\".format(min_delta))\n",
    "        # When the difference at its lowest, it means we find value\n",
    "        #  of the 'value' for which we reach the targeted 'target quantil'\n",
    "        if delta <= min_delta:\n",
    "            min_delta = delta\n",
    "            found_value = moving_value\n",
    "            found_qtil = moving_qtil\n",
    "            if __debug_script__:\n",
    "                print('    change of delta')\n",
    "    if __debug_script__:\n",
    "        print('    value:', found_value)\n",
    "\n",
    "    del df, c_name, tgt_qtil, data, delta, min_delta,\\\n",
    "        moving_value_int, moving_value, moving_qtil\n",
    "    return found_qtil, found_value\n",
    "\n",
    "\n",
    "def get_basic_stats(df, c_name):\n",
    "    '''\n",
    "    returns a dictionary with stats title and results\n",
    "\n",
    "     input:\n",
    "       df      : dataframe to cope with\n",
    "       c_name: name of the df's column to work on\n",
    "     output:\n",
    "       stats: dictionnary with 'maximum', 'average' and 'median' values \n",
    "              of the selected dataset.\n",
    "    '''\n",
    "    # Function: Compute and display basic stats on a given column of a df\n",
    "\n",
    "    column = df[c_name] # Get values of the divide\n",
    "    # Compute ratio of computed result 'c_name'\n",
    "    #  over the full size of data\n",
    "    ratio_valid_result = 100 * column.isna().sum() / len(df)\n",
    "    msg1, msg2 = '- Stats computed on', 'of the full dataset'\n",
    "    print('   ', msg1, \"{:.2f}\".format(ratio_valid_result), '% (', \"{:d}\".format(column.isna().sum()), 'values)', msg2)\n",
    "    maximum_value = column.max()\n",
    "    median_value = column.median()\n",
    "    average_value = column.mean()\n",
    "    print('    - maximum:', \"{:.2f}\".format(maximum_value), '%',\n",
    "            '\\n    - average:', \"{:.2f}\".format(average_value), '%',\n",
    "            '\\n    - median :', \"{:.2f}\".format(median_value), '%')\n",
    "    \n",
    "    del df, c_name, column, ratio_valid_result, msg1, msg2\n",
    "    return {'maximum': maximum_value, 'average': average_value, 'median': median_value}\n",
    "\n",
    "\n",
    "def get_rate_stats(df, c_name, tgt_pct):\n",
    "    '''\n",
    "    returns a dictionary of quantil targets\n",
    "\n",
    "    input:\n",
    "      df      : dataframe to cope with\n",
    "      c_name: name of the df's column to work on\n",
    "      tgt_pct : list of target percentage values of the population\n",
    "                to consider, spreading most critical cases\n",
    "    output:\n",
    "      stats: dictionnary with 'maximum', 'average' and 'median' values \n",
    "             of the selected dataset.\n",
    "    '''\n",
    "    # Function: Compute and display stats values of the selected category\n",
    "    #            for a couple of target quantil\n",
    "\n",
    "    # output\n",
    "    stats = {}\n",
    "    # sub-variables\n",
    "    tgt_rate, value_of_qtil, value_at_qtil = None, None, None\n",
    "\n",
    "    # Give quantity of valid data\n",
    "    column_data = df[c_name]\n",
    "    nb_valid_data = len(df) - column_data.isna().sum()\n",
    "\n",
    "    # We give target quantil and we want to know the value of the c_name \n",
    "    #  where we met the closest the target quantil.\n",
    "    #  For instance, we define a target at 95.4% (2 sigma) and we want\n",
    "    #  to know value of the divide values where 95.4% of the values\n",
    "    #  are equal or below this target. Because the sample's distribution\n",
    "    #  is not continue and maybe big enough, the % may be close but not\n",
    "    #  exactly at the target quantil.\n",
    "    # Make the computation for all targets defined in the list tgt_pct:\n",
    "    for tgt_rate in tgt_pct:\n",
    "        # Try reaching a quantil as close as possible of the target\n",
    "        #  a return the associated value at the quantil reached.\n",
    "        value_of_qtil, value_at_qtil = get_value_at_tgt_quantil(df, c_name, tgt_rate, nb_valid_data)\n",
    "        \n",
    "        # Record the result into a dictionay\n",
    "        stats[tgt_rate] = {'quantil': value_of_qtil, 'value': value_at_qtil}\n",
    "\n",
    "        # Display the reached (vs target) population rate spreading \n",
    "        #  the critical cases and the related rate according to the\n",
    "        #  categories used in the divide.\n",
    "        print('    - It may concern', \"{:.2f}\".format(value_at_qtil), '%',\n",
    "              'of COVID-19 patients'\n",
    "              ' when considering', \"{:.2f}\".format(value_of_qtil), '%',\n",
    "              'of the less critical patients',\n",
    "              '(for an initial target at', \"{:.2f}\".format(tgt_rate), '%)')\n",
    "\n",
    "    del df, c_name, tgt_pct, nb_valid_data, tgt_rate, column_data\n",
    "    return stats\n",
    "\n",
    "\n",
    "def get_selected_rate(df, input_dict, min_sample_size, tgt_pct):\n",
    "    '''\n",
    "    returns a dictionary of pre-defined stats values about the \n",
    "     selected data in a dataframe\n",
    "\n",
    "    input:\n",
    "        df          : dataframe to cope with\n",
    "     input_dict     : dictionnary with result's titles as keys and\n",
    "                       a list with df's column names for numerator\n",
    "                       and denominator as values\n",
    "                       = {title1: [col_N1, col_D1], title2: [...] ...}\n",
    "    min_sample_size : Minimum value to consider on denominator do perform\n",
    "                       the computation\n",
    "      tgt_pct       : list of target percentage values of the population\n",
    "                       to consider, spreading most critical cases\n",
    "    output:\n",
    "      outputs       : dictionary with result's titles as keys and\n",
    "                       a sub-dictionary with target percentage and\n",
    "                       result values\n",
    "    '''\n",
    "    # function: provide stats (max, average, median) and find value at\n",
    "    #  three target quantil on a category of dataframe over another.\n",
    "\n",
    "    # output\n",
    "    outputs = {}\n",
    "\n",
    "    # sub-variables\n",
    "    stats_basic, stats_rate = {}, {}\n",
    "    title = None\n",
    "\n",
    "    for title in input_dict:\n",
    "\n",
    "        # Second step of the stats consists in looking for the value (%)\n",
    "        #  of transfer of the Covid-19 patients from one state to another\n",
    "\n",
    "         # get column's title of [numerator; denominator]\n",
    "        num_denum = input_dict[title]\n",
    "        # Define name of the df's column from defined title of results\n",
    "        result_name = title.replace(\" \", \"_\")\n",
    "        # Perform the divide of Numerator by Denominator\n",
    "        df = divide_N_by_D(df, num_denum, result_name, min_sample_size)\n",
    "\n",
    "        print(\"  > Provide stats related to\", title, \":\")\n",
    "\n",
    "        # First stats are basic stats.\n",
    "        # I call a function to get basic stats on this new computed values\n",
    "        stats_basic = get_basic_stats(df, result_name)\n",
    "\n",
    "        # Second step of the stats consists in looking for the value (%)\n",
    "        #  of transfer of the Covid-19 patients from one state to another\n",
    "        # For that, I call a function to get stats on given proportion \n",
    "        #  of the considered population of patients\n",
    "        stats_rate = get_rate_stats(df, result_name, tgt_pct)\n",
    "\n",
    "        # Implement result of stats on the divide into output dict.\n",
    "        outputs[title] = title  # feed the result dict with inputs\n",
    "        outputs[title] = stats_rate  # ... and with the stats results\n",
    "\n",
    "    del df, input_dict, min_sample_size, tgt_pct, num_denum,\\\n",
    "        result_name, title\n",
    "    return outputs\n",
    "\n",
    "\n",
    "# Note literally the requested rates and associated categories \n",
    "#  of the dataframe to compute the requested stats\n",
    "inputs = {'rate of patients in critical care over hospitalization':\n",
    "          ['Nb_actuellement_en_soins_intensifs', 'Nb_actuellement_hospitalises'],\n",
    "          'rate of female that died due to covid-19': ['Sexe_Femme', 'Total_Deces'],\n",
    "          'rate of male that died due to covid-19': ['Sexe_Homme', 'Total_Deces']}\n",
    "min_sample_size = 10  # minimum sample size, below this quantity of available values,\n",
    "                      #  the data are not considered for computing stats.\n",
    "# According to Gaussian theory: %-values refer to 3sigma, 2sigma, 1sigma\n",
    "tgt_pct = [99.7, 95.4, 68.3]\n",
    "# I've prepared a set of three entries to do the same operation of three\n",
    "#  different factors. I send these entries in a first function that will apply the \n",
    "# same process on a loop. The unitary processing will go a step deeper.\n",
    "stats_result = get_selected_rate(df, inputs, min_sample_size, tgt_pct)\n",
    "\n",
    "# END OF THE PROJECT\n",
    "del inputs, min_sample_size, stats_result\n",
    "\n",
    "# RESULT\n",
    "# Processing time is a bit long, notably because the script\n",
    "#  realizes three times a loop between 0 to 100 with 0.01-step\n",
    "#  A better way to code is very probably possible, but here, the time \n",
    "#  is not a key factor for the evaluation\n",
    "# Concerning Covid-19 patients (13.8% of reported case), when the 5%-most critical\n",
    "#  cases are spread (i.e. the 95.4% value = 2sigma):\n",
    "# - Transfer rate from hospitalization to critical care is about 30%; So 70% of luck\n",
    "#   to avoid critical care\n",
    "# - Mortality is about 2.4% for female patients and 1.9% for male patients;\n",
    "#   so respectively 97.6% and 98.1% of chance avoiding death.\n",
    "\n",
    "print('\\n' + 10 * \"-\", \"PROJECT #01 /  END \", 10 * \"-\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit (system)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
